[["index.html", "Stats notes Ramblings", " Stats notes Koen van Benthem 22-06-23 Ramblings "],["introduction.html", "1 Introduction", " 1 Introduction These pages serve as a location to store notes that I have made on statistics. Some of it might be reliable, some of it might not. It is mainly intended for my own use, but if you stumble upon these pages and want to explore - feel free! "],["differences-between-two-groups.html", "2 Differences between two groups 2.1 Notation 2.2 t-test 2.3 ANOVA 2.4 linear model", " 2 Differences between two groups Here, I try to show that ANOVA, two sample t-test and lm return the same result when the data only consist of two groups. The recipes for ANOVA, t-test and lm were taken from Whitlock and Schluter (2015) . 2.1 Notation Assume we have an explanatory variable \\(x\\) that was only measured at two specific values: \\(A\\) and \\(B\\) (e.g. \\(x\\) was temperature and only two different values, e.g. \\(A=18^\\circ C\\) and \\(B=21^\\circ C\\)). In order to simplify the notation, the datapoints have been ordered, such that the first \\(N_A\\) datapoints have \\(x=A\\) and the last \\(N_B\\) have \\(x=B\\), with a total number of data points \\(N=N_A+N_B\\). We assume the response variable \\(y\\) to be normally distributed. It may for example represent body size. We note that the group specific means of \\(y\\) are: \\[\\begin{equation} \\overline{y_A} = \\frac{1}{N_A}\\sum_{i=1}^{N_A} y_i \\end{equation}\\] \\[\\begin{equation} \\overline{y_B} = \\frac{1}{N_B}\\sum_{i=N_A+1}^{N} y_i \\end{equation}\\] and the overall mean: \\[\\begin{equation} \\overline{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i \\end{equation}\\] 2.2 t-test For a simple t-test, we first have to calculate the standard error, which in turn depends on the pooled sample variance, which in turn depends on the separate variances. We denote the variances for group A and B as: \\[\\begin{align} s_A^2 &amp;= \\sum_{i=1}^{N_A}\\frac{(y_i-\\overline{y_A})^2}{N_A-1} \\\\ s_B^2 &amp;= \\sum_{i=N_A+1}^{N_B}\\frac{(y_i-\\overline{y_B})^2}{N_B-1} \\end{align}\\] The pooled sample variance is then: \\[\\begin{equation} s_p^2 = \\frac{\\text{df}_As_A^2 + \\text{df}_Bs_B^2}{\\text{df}_A+\\text{df}_B} = \\frac{\\sum_{i=1}^{N_A}(N_A-1)\\frac{(y_i-\\overline{y_A})^2}{N_A-1} + \\sum_{i=N_A+1}^{N_B}(N_B-1)\\frac{(y_i-\\overline{y_B})^2}{N_B-1} }{N - 2} \\end{equation}\\] The standard error is then: \\[\\begin{equation} \\text{SE}_{\\overline{y_A}-\\overline{y_B}} = \\sqrt{s_p^2\\left(\\frac{1}{N_A}+\\frac{1}{N_B}\\right)} = \\sqrt{\\frac{\\sum_{i=1}^{N_A}(y_i-\\overline{y_A})^2 + \\sum_{i=N_A+1}^{N_B}(y_i-\\overline{y_B})^2 }{N - 2} \\left(\\frac{1}{N_A}+\\frac{1}{N_B}\\right)} \\end{equation}\\] the test statistic is: \\[\\begin{equation} t=\\frac{\\overline{y_A}-\\overline{y_B}}{\\text{SE}_{\\overline{y_A}-\\overline{y_B}}} = \\sqrt{\\frac{N_AN_B(N-2)(\\overline{y_A}-\\overline{y_B})^2}{N\\left(\\sum_{i=1}^{N_A}(y_i-\\overline{y_A})^2 + \\sum_{i=N_A+1}^{N_B}(y_i-\\overline{y_B})^2\\right)}} \\end{equation}\\] 2.3 ANOVA We first calculate the group sum of squares: \\[\\begin{equation} \\text{SS}_\\text{groups} = N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2 \\end{equation}\\] as well as the error sum of squares: \\[\\begin{equation} \\text{SS}_\\text{error} = \\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2 \\end{equation}\\] From here, we can easily obtain the \\(F\\)-statistic as: \\[\\begin{equation} F = \\frac{(N-2)\\left(N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2\\right)}{\\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2} \\end{equation}\\] This already looks a bit like \\(t\\), but in order to see the equivalency between the two, we have a bit more rewriting to do. Specifically, we will focus on: \\[\\begin{align} N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2 &amp;= N_A\\overline{y_A}^2 - 2N_A \\overline{y_A}\\overline{y} + N_A\\overline{y}^2 + N_B\\overline{y_B}^2 - 2N_B\\overline{y_B}\\overline{y} + N_B \\overline{y}^2 \\\\ &amp;= N_A\\overline{y_A}^2 - 2(N_A \\overline{y_A}+N_B\\overline{y_B})\\overline{y} + (N_A+N_B)\\overline{y}^2 + N_B\\overline{y_B}^2\\\\ &amp;=N_A\\overline{y_A}^2 - \\frac{(N_A \\overline{y_A}+N_B\\overline{y_B})^2}{N} + N_B\\overline{y_B}^2\\\\ &amp;= N_A\\overline{y_A}^2 - \\frac{N_A^2 \\overline{y_A}^2+N_B^2\\overline{y_B}^2 + 2N_AN_B\\overline{y_A}\\overline{y_B}}{N} + N_B\\overline{y_B}^2\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\frac{N}{N_B}\\overline{y_A}^2 - \\frac{N_A}{N_B} \\overline{y_A}^2-\\frac{N_B}{N_A}\\overline{y_B}^2 - 2\\overline{y_A}\\overline{y_B} + \\frac{N}{N_A}\\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\frac{N-N_A}{N_B}\\overline{y_A}^2 - 2\\overline{y_A}\\overline{y_B} + \\frac{N-N_B}{N_A}\\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\overline{y_A}^2 - 2\\overline{y_A}\\overline{y_B} + \\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\overline{y_A} - \\overline{y_B}\\right)^2\\\\ \\end{align}\\] Where in going from the second to the third line, we have used that \\(N\\overline{y}=N_A\\overline{y_A}+N_B\\overline{y_B}\\), and on going from the sixt to the seventh line that \\(N-N_A = N_B\\) and equivalently \\(N-N_B= N_A\\). We can not use this identity to rewrite our \\(F\\)-statistic: \\[\\begin{equation} F = \\frac{(N-2)N_AN_B\\left(\\overline{y_A}-\\overline{y_B}\\right)^2}{N\\left(\\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2\\right)} = t^2 \\end{equation}\\] and hence \\(F=t^2\\). 2.4 linear model Using the formula in Whitlock and Schluter (2015) , we can determine the slope for the line between the points as: \\[\\begin{equation} b=\\frac{\\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N (x_i - \\overline{x})^2} \\end{equation}\\] with standard error: \\[\\begin{equation} \\text{SE}_b = \\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-b\\sum_{i=1}^N (x_i-\\overline{x})(y_i - \\overline{y})}{(N-2)\\sum_{i=1}^N (x_i-\\overline{x})^2}} \\end{equation}\\] In order to simplify this for the specific case with only two possible values of x, we first focus on the following simpler expressions: \\[\\begin{align} \\overline{x} &amp;= \\frac{N_A A + N_B B}{N} \\end{align}\\] We can use this to write: \\[\\begin{align} \\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y}) &amp;= \\sum_{i=1}^{N_A} (A - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) + \\sum_{i=N_A+1}^{N} (B - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) \\\\ &amp;= \\sum_{i=1}^{N_A} (\\frac{NA}{N} - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) + \\sum_{i=N_A+1}^{N} (\\frac{NB}{N} - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) \\\\ &amp;= \\frac{1}{N}\\left(\\sum_{i=1}^{N_A} N_B (A - B)(y_i - \\overline{y}) + \\sum_{i=N_A+1}^{N} N_A (B - A)(y_i - \\overline{y})\\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(\\sum_{i=1}^{N_A} N_B (y_i - \\overline{y}) - \\sum_{i=N_A+1}^{N} N_A (y_i - \\overline{y})\\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(N_B \\sum_{i=1}^{N_A} y_i - N_BN_A\\overline{y} - N_A \\sum_{i=N_A+1}^{N} y_i + N_AN_B \\overline{y}\\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(N_B \\sum_{i=1}^{N_A} y_i - N_A \\sum_{i=N_A+1}^{N} y_i \\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(N_BN_A \\overline{y_A} - N_A N_B \\overline{y_B}\\right) \\\\ &amp;= \\frac{(A - B)N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right) \\\\ \\end{align}\\] Similarly, we attempt to simplify the term: \\[\\begin{align} \\sum_{i=1}^N (x_i - \\overline{x})^2 &amp;= \\sum_{i=1}^N (x_i - \\frac{N_A A + N_B B}{N})^2 \\\\ &amp;= \\sum_{i=1}^{N_A} (A - \\frac{N_A A + N_B B}{N})^2 + \\sum_{i=N_A+1}^N (B - \\frac{N_A A + N_B B}{N})^2 \\\\ &amp;= \\sum_{i=1}^{N_A} (\\frac{NA}{N} - \\frac{N_A A + N_B B}{N})^2 + \\sum_{i=N_A+1}^N (\\frac{NB}{N} - \\frac{N_A A + N_B B}{N})^2 \\\\ &amp;= \\sum_{i=1}^{N_A} (\\frac{N_B (A - B)}{N})^2 + \\sum_{i=N_A+1}^N (\\frac{N_A (B-A)}{N})^2 \\\\ &amp;= N_A (\\frac{N_B (A - B)}{N})^2 + N_B (\\frac{N_A (B-A)}{N})^2 \\\\ &amp;= (A-B)^2\\left(N_A \\frac{N_B^2 }{N^2} + N_B \\frac{N_A^2 }{N^2}\\right) \\\\ &amp;= (A-B)^2\\left( \\frac{N_AN_B^2 + N_B N_A^2 }{(N_A+N_B)^2}\\right) \\\\ &amp;= (A-B)^2\\left( \\frac{N_AN_B (N_A + N_B)}{(N_A+N_B)^2}\\right) \\\\ &amp;= (A-B)^2 \\frac{N_AN_B }{N} \\\\ \\end{align}\\] After a lot of rewriting, we can now use these expressions to find a much expected answer: \\[\\begin{align} b&amp;=\\frac{\\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N (x_i - \\overline{x})^2}\\\\ &amp;=\\frac{\\overline{y_A}-\\overline{y_B}}{A-B} \\end{align}\\] in other words: when we have only two distinct values for the \\(x\\)-variable, the slope is equal to the difference in the means in \\(y\\) between these to groups and the difference between the \\(x\\)-values. This result was of course completely to be expected, and we could have obtained it through reasoning alone, without the need to rewrite these equations at length. The next step is to rewrite the standard error for the slope: \\[\\begin{align} \\text{SE}_b &amp;= \\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-b\\sum_{i=1}^N (x_i-\\overline{x})(y_i - \\overline{y})}{(N-2)\\sum_{i=1}^N (x_i-\\overline{x})^2}} \\\\ &amp;= \\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-\\frac{\\overline{y_A}-\\overline{y_B}}{A-B}\\frac{(A - B)N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right)}{(N-2)(A-B)^2 \\frac{N_AN_B}{N}}} \\\\ &amp;= \\frac{1}{A-B}\\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-\\frac{N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right)^2}{(N-2) \\frac{N_AN_B}{N}}} \\end{align}\\] Now, the test statistic for testing whether \\(b\\) is equal to zero is (\\(t_b\\)): \\[\\begin{align} t_b &amp;= \\frac{b}{\\text{SE}_b}\\\\ &amp;= \\sqrt{\\frac{(N-2) \\frac{N_AN_B}{N}(\\overline{y_A}-\\overline{y_B})^2}{\\sum_{i=1}^N (y_i-\\overline{y})^2-\\frac{N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right)^2}} \\\\ &amp;= \\sqrt{\\frac{(N-2) N_AN_B(\\overline{y_A}-\\overline{y_B})^2}{N\\sum_{i=1}^N (y_i-\\overline{y})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2}} \\end{align}\\] Let’s first rewrite the following: \\[\\begin{align} N&amp;\\sum_{i=1}^N (y_i-\\overline{y})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2 = N\\sum_{i=1}^N (y_i^2-2y_i\\overline{y} + \\overline{y}^2)-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^N y_i^2-2N^2\\overline{y}^2 + N^2\\overline{y}^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^N y_i^2-(N\\overline{y})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^{N} y_i^2 -(N_A\\overline{y_A}+N_B\\overline{y_B})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^{N} y_i^2 -N_A^2\\overline{y_A}^2-N_B^2\\overline{y_B}^2-2N_AN_B\\overline{y_A}\\overline{y_B}-N_AN_B\\overline{y_A}^2 - N_AN_B\\overline{y_B}^2 + 2N_AN_B\\overline{y_A}\\overline{y_B}\\\\ &amp;= N\\sum_{i=1}^{N} y_i^2 -N_A(N_A+N_B)\\overline{y_A}^2-N_B(N_B+N_A)\\overline{y_B}^2 \\\\ &amp;= N\\left(\\sum_{i=1}^{N} y_i^2 -N_A\\overline{y_A}^2-N_B\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N} y_i^2 -2N_A\\overline{y_A}^2+N_A\\overline{y_A}^2-2N_B\\overline{y_B}^2+N_B\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N} y_i^2 -\\sum_{i=1}^{N_A} 2\\overline{y_A}y_i+\\sum_{i=1}^{N_A}\\overline{y_A}^2-2\\sum_{i=N_A+1}^N\\overline{y_B}y_i+\\sum_{i=N_B+1}^N\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N_A} y_i^2 + \\sum_{i=N_A+1}^{N} y_i^2 -\\sum_{i=1}^{N_A} 2\\overline{y_A}y_i+\\sum_{i=1}^{N_A}\\overline{y_A}^2-2\\sum_{i=N_A+1}^N\\overline{y_B}y_i+\\sum_{i=N_B+1}^N\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N_A} \\left(y_i^2 - 2\\overline{y_A}y_i+ \\overline{y_A}^2 \\right) + \\sum_{i=N_A+1}^{N} \\left(y_i^2 -2\\overline{y_B}y_i+\\overline{y_B}^2 \\right)\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N_A} \\left(y_i - \\overline{y_A} \\right)^2 + \\sum_{i=N_A+1}^{N} \\left(y_i -\\overline{y_B}\\right)^2\\right) \\\\ \\end{align}\\] By using this simplification, we can rewrite the value of the \\(t\\)-statistic as: \\[\\begin{equation} t_b = \\sqrt{\\frac{(N-2) N_AN_B(\\overline{y_A}-\\overline{y_B})^2}{ N\\left(\\sum_{i=1}^{N_A} \\left(y_i - \\overline{y_A} \\right)^2 + \\sum_{i=N_A+1}^{N} \\left(y_i -\\overline{y_B}\\right)^2\\right)}} = t = \\sqrt{F} \\end{equation}\\] and hence we conclude that \\(t=t_b\\). Bibliography Whitlock, Michael, and Dolph Schluter. 2015. The Analysis of Biological Data. Vol. 768. Roberts Publishers Greenwood Village, Colorado. "],["correlation-test-and-t-test.html", "3 Correlation test and t-test 3.1 Notation 3.2 Linear model 3.3 Correlation test", " 3 Correlation test and t-test Here, I try to show that a correlation test and t-test on the slope of a regression analysis (single linear regression) give identical results. The recipes for the two tests were taken from Whitlock and Schluter (2015) . 3.1 Notation Assume we have an explanatory variable \\(x\\) and a response variable \\(y\\). The former might for example be average foraging rate and the latter body size. In total we have \\(N\\) observations, with \\(x_i\\) the value \\(x\\) for individual \\(i\\) and \\(y_i\\) the value of \\(y\\) for individual \\(i\\). The average values of \\(x\\) and \\(y\\) are given by \\(\\overline{x}\\) and \\(\\overline{y}\\) respectively. Throughout the following we use the following definitions for variance (\\(\\sigma_x^2\\), \\(\\sigma_y^2\\)) and covariance (\\(\\sigma_{x,y}\\)): \\[\\begin{align} \\sigma_x^2 &amp;= \\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N-1}\\\\ \\sigma_y^2 &amp;= \\frac{\\sum_{i=1}^N (y_i - \\overline{y})^2}{N-1}\\\\ \\sigma_{x,y} &amp;= \\frac{\\sum_{i=1}^N (x_i - \\overline{x})(y_i - \\overline{y})}{N-1}\\\\ \\end{align}\\] 3.2 Linear model Using the formula in Whitlock and Schluter (2015) , we can determine the slope for the line between the points as: \\[\\begin{equation} b=\\frac{\\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N (x_i - \\overline{x})^2} = \\frac{\\sigma_{x,y}}{\\sigma_x^2} \\end{equation}\\] with standard error: \\[\\begin{align} \\text{SE}_b &amp;= \\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-b\\sum_{i=1}^N (x_i-\\overline{x})(y_i - \\overline{y})}{(N-2)\\sum_{i=1}^N (x_i-\\overline{x})^2}}\\\\ &amp;= \\sqrt{\\frac{\\sigma_y^2-b\\sigma_{x,y}}{(N-2)\\sigma_x^2}}\\\\ &amp;= \\sqrt{\\frac{\\sigma_x^2\\sigma_y^2-\\sigma_{x,y}^2}{(N-2)\\sigma_x^4}}\\\\ \\end{align}\\] and t-statistic (\\(t_b\\)): \\[\\begin{align} t_b = \\frac{b}{\\text{SE}_b} &amp;= \\sqrt{\\frac{\\sigma_{x,y}^2}{\\sigma_x^4}\\frac{(N-2)\\sigma_x^4}{\\sigma_x^2\\sigma_y^2-\\sigma_{x,y}^2}}\\\\ &amp;= \\sqrt{\\frac{(N-2)\\sigma_{x,y}^2}{\\sigma_x^2\\sigma_y^2-\\sigma_{x,y}^2}} \\end{align}\\] 3.3 Correlation test the correlation coefficient \\(r\\) is equals (Whitlock and Schluter 2015): \\[\\begin{equation} r = \\frac{\\sum_{i=1}^N (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^N (x_i - \\overline{x})^2}\\sqrt{\\sum_{i=1}^N (y_i - \\overline{y})^2}} = \\frac{\\sigma_{x,y}}{\\sigma_x\\sigma_y} \\end{equation}\\] this regression coefficient has an associated standard error (\\(\\text{SE}_r\\)) of: \\[\\begin{align} \\text{SE}_r &amp;= \\sqrt{\\frac{1-r^2}{N-2}}\\\\ &amp;= \\sqrt{\\frac{1-\\frac{\\sigma_{x,y}^2}{\\sigma_x^2\\sigma_y^2}}{N-2}}\\\\ &amp;= \\sqrt{\\frac{\\sigma_x^2\\sigma_y^2-\\sigma_{x,y}^2}{(N-2)\\sigma_x^2\\sigma_y^2}}\\\\ \\end{align}\\] the corresponding t-statistic is then: \\[\\begin{align} t_r = \\frac{r}{\\text{SE_r}} &amp;= \\sqrt{\\frac{\\sigma_{x,y}^2}{\\sigma_x^2\\sigma_y^2}\\frac{(N-2)\\sigma_x^2\\sigma_y^2}{\\sigma_x^2\\sigma_y^2-\\sigma_{x,y}^2}} \\\\ &amp;= \\sqrt{\\frac{(N-2)\\sigma_{x,y}^2}{\\sigma_x^2\\sigma_y^2-\\sigma_{x,y}^2}} \\\\ &amp;= t_b \\end{align}\\] Hence, both methods should give identical results. Bibliography Whitlock, Michael, and Dolph Schluter. 2015. The Analysis of Biological Data. Vol. 768. Roberts Publishers Greenwood Village, Colorado. "],["r2-and-correlation.html", "4 \\(R^2\\) and correlation 4.1 Notation 4.2 Correlation between \\(x\\) and \\(y\\) 4.3 \\(R^2\\) value of the linear model 4.4 Correlation between \\(y\\) and \\(\\hat{y}\\) 4.5 \\(R^2\\) of a linear regression of \\(y\\) on \\(\\hat{y}\\) 4.6 \\(R^2\\) for multiple regression 4.7 \\(R^2\\) value of more general models", " 4 \\(R^2\\) and correlation For this section, we assume a simple data set that consists of a response variable, \\(y\\) and an explanatory variable \\(x\\). Here, I try to show the relationship between the following metrics: The correlation between \\(x\\) and \\(y\\) The \\(R^2\\) value of the regression model \\(y_i = a + bx_i + \\epsilon_i\\), with \\(\\epsilon_i\\) a residual error from a normal distribution. The correlation between \\(y\\) and \\(\\hat{y}\\), the latter being the predictions from the regression model under 2. The \\(R^2\\) value of the regression model \\(y_i = a&#39; + b&#39;\\hat{y}_i + \\epsilon&#39;_i\\). 4.1 Notation Assume we have an explanatory variable \\(x\\) and a response variable \\(y\\). In total we have \\(N\\) observations, with \\(x_i\\) the value \\(x\\) for individual \\(i\\) and \\(y_i\\) the value of \\(y\\) for individual \\(i\\). The average values of \\(x\\) and \\(y\\) are given by \\(\\overline{x}\\) and \\(\\overline{y}\\) respectively. Throughout the following we use the following definitions for variance (\\(\\sigma_x^2\\), \\(\\sigma_y^2\\)) and covariance (\\(\\sigma_{x,y}\\)): \\[\\begin{align} \\sigma_x^2 &amp;= \\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N-1}\\\\ \\sigma_y^2 &amp;= \\frac{\\sum_{i=1}^N (y_i - \\overline{y})^2}{N-1}\\\\ \\sigma_{x,y} &amp;= \\frac{\\sum_{i=1}^N (x_i - \\overline{x})(y_i - \\overline{y})}{N-1}\\\\ \\end{align}\\] 4.2 Correlation between \\(x\\) and \\(y\\) the correlation coefficient \\(r\\) equals (Whitlock and Schluter 2015): \\[\\begin{equation} r = \\frac{\\sum_{i=1}^N (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^N (x_i - \\overline{x})^2}\\sqrt{\\sum_{i=1}^N (y_i - \\overline{y})^2}} = \\frac{\\sigma_{x,y}}{\\sigma_x\\sigma_y} \\end{equation}\\] 4.3 \\(R^2\\) value of the linear model Using the formula in Whitlock and Schluter (2015) , we can determine the intercept and slope for the regression line as: \\[\\begin{align} b &amp;= \\frac{\\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N (x_i - \\overline{x})^2} &amp;= \\frac{\\sigma_{x,y}}{\\sigma_x^2} \\\\ a &amp;= \\overline{y} - b \\overline{x} &amp;= \\overline{y} - \\frac{\\sigma_{x,y}}{\\sigma_x^2} \\overline{x}\\\\ \\end{align}\\] Next, we define the formula for \\(R^2\\), which is: \\[\\begin{equation} R^2 = 1 - \\frac{\\sum_i (\\hat{y_i}-y_i)^2}{\\sum_i (\\overline{y} - y_i)^2} \\end{equation}\\] With \\(\\hat{y_i}=a + b x_i\\), this formula becomes: \\[\\begin{align} R^2 &amp;= 1 - \\frac{\\sum_i (a+b x_i-y_i)^2}{\\sum_i (\\overline{y} - y_i)^2}\\\\ &amp;= 1 - \\frac{\\sum_i (\\overline{y} - \\frac{\\sigma_{x,y}}{\\sigma_x^2}\\overline{x} + \\frac{\\sigma_{x,y}}{\\sigma_x^2} x_i-y_i)^2}{\\sum_i (\\overline{y} - y_i)^2}\\\\ &amp;= 1 - \\frac{\\sum_i (\\overline{y} -y_i + \\frac{\\sigma_{x,y}}{\\sigma_x^2} (x_i-\\overline{x}))^2}{\\sum_i (\\overline{y} - y_i)^2}\\\\ &amp;= 1 - \\frac{\\sum_i (\\overline{y} -y_i)^2 + 2\\frac{\\sigma_{x,y}}{\\sigma_x^2} (x_i-\\overline{x})(\\overline{y} -y_i) + \\frac{\\sigma_{x,y}^2}{\\sigma_x^4} (x_i-\\overline{x})^2}{\\sum_i (\\overline{y} - y_i)^2}\\\\ &amp;= - \\frac{ 2\\frac{\\sigma_{x,y}}{\\sigma_x^2} \\sum_i (x_i-\\overline{x})(\\overline{y} -y_i) + \\frac{\\sigma_{x,y}^2}{\\sigma_x^4} \\sum_i (x_i-\\overline{x})^2}{\\sum_i (\\overline{y} - y_i)^2}\\\\ &amp;= \\frac{ 2\\frac{\\sigma_{x,y}}{\\sigma_x^2} \\sum_i (x_i-\\overline{x})(y_i - \\overline{y}) - \\frac{\\sigma_{x,y}^2}{\\sigma_x^4} \\sum_i (x_i-\\overline{x})^2}{\\sum_i (\\overline{y} - y_i)^2} \\end{align}\\] Now we divide both numerator and denominator by \\(N-1\\) to find: \\[\\begin{align} R^2 &amp;= \\frac{ 2\\frac{\\sigma_{x,y}}{\\sigma_x^2} \\sigma_{x,y} - \\frac{\\sigma_{x,y}^2}{\\sigma_x^4} \\sigma_x^2}{\\sigma_y^2}\\\\ &amp;= \\frac{ \\sigma_{x,y}^2}{\\sigma_x^2\\sigma_y^2}\\\\ &amp;= r^2 \\end{align}\\] Hence, for linear regression the \\(R^2\\) value, is identical to the square of the correlation coefficient. This is a very commonly known result. 4.4 Correlation between \\(y\\) and \\(\\hat{y}\\) Now, we focus on a slightly different correlation (\\(r&#39;\\)), the one between the predictions of the linear model on one hand, and the actual y values on the other. The formula for this correlation should be: \\[\\begin{equation} r&#39; = \\frac{\\sum_{i=1}^N (\\hat{y}_i - \\overline{\\hat{y}})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^N (\\hat{y}_i - \\overline{\\hat{y}})^2}\\sqrt{\\sum_{i=1}^N (y_i - \\overline{y})^2}} \\end{equation}\\] Now we need to simplify this. We will first start by finding an expression for the average prediction, \\(\\overline{\\hat{y}}\\), which intuitively should be identical to the average value of \\(y\\). We write the expression out and simplify: \\[\\begin{align} \\overline{\\hat{y}} &amp;= \\frac{1}{N} \\sum_i (a+bx_i)\\\\ &amp;= \\frac{1}{N} \\sum_i (\\overline{y} - \\frac{\\sigma_{x,y}}{\\sigma_x^2}\\overline{x}+\\frac{\\sigma_{x,y}}{\\sigma_x^2} x_i) \\\\ &amp;= \\overline{y} \\end{align}\\] And thus: \\[\\begin{equation} r&#39; = \\frac{\\sum_{i=1}^N (\\hat{y}_i - \\overline{y})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^N (\\hat{y}_i - \\overline{y})^2}\\sqrt{\\sum_{i=1}^N (y_i - \\overline{y})^2}} \\end{equation}\\] Now we use the earlier found expression that \\(\\hat{y_i} = a + bx_i = \\overline{y} - \\frac{\\sigma_{x,y}}{\\sigma_x^2}\\overline{x}+\\frac{\\sigma_{x,y}}{\\sigma_x^2} x_i = \\overline{y} +\\frac{\\sigma_{x,y}}{\\sigma_x^2} (x_i-\\overline{x})\\) to rewrite the expression of \\(r&#39;\\) to be: \\[\\begin{align} r&#39; &amp;= \\frac{\\sum_{i=1}^N \\frac{\\sigma_{x,y}}{\\sigma_x^2} (x_i-\\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^N (\\frac{\\sigma_{x,y}}{\\sigma_x^2} (x_i-\\overline{x}))^2}\\sqrt{\\sum_{i=1}^N (y_i - \\overline{y})^2}} \\\\ &amp;= \\frac{\\sum_{i=1}^N (x_i-\\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^N (x_i-\\overline{x})^2}\\sqrt{\\sum_{i=1}^N (y_i - \\overline{y})^2}} \\\\ &amp;= r \\end{align}\\] 4.5 \\(R^2\\) of a linear regression of \\(y\\) on \\(\\hat{y}\\) We have shown before that if the correlation coefficient of two variables is r, the \\(R^2\\) value that is obtained by regressing one variable on the other is equal to \\(r^2\\). Therefore, the \\(R^2\\) of the regression of \\(y\\) on \\(\\hat{y}\\) is identical to \\(r^2\\). The result shown here holds for simple regressions. We can check these behaviours with a quick simulation: N &lt;- 10 # number of data points x &lt;- runif(10) y &lt;- 2.1*x + 0.6 + rnorm(N) # 1. correlation squared cor(x,y)^2 ## [1] 0.3352486 # 2. r squared of linear model m1 &lt;- lm(y~x) summary(m1)$r.squared ## [1] 0.3352486 # 3. correlation predictions and real values for y: ypred &lt;- predict(m1) cor(y,ypred)^2 ## [1] 0.3352486 # 4. R squared linear model y~ypred summary(lm(y~ypred))$r.squared ## [1] 0.3352486 We note that all values are identical. 4.6 \\(R^2\\) for multiple regression Without proof, it seems that the \\(R^2\\) value of a multiple regression is identical to the correlation between the response variables and their predicted values by the multiple regression. For example: N &lt;- 50 x1 &lt;- runif(N) x2 &lt;- runif(N) x3 &lt;- runif(N) y &lt;- x1 + 2.1*x2 - 0.8*x3 + 4.47*x2*x3 + 5.6 + rnorm(N) m1 &lt;- lm(y~x1+x2+x3+x2:x3) summary(m1)$r.squared ## [1] 0.5579047 cor(y,predict(m1))^2 ## [1] 0.5579047 We can repeat this process many times to see how general it is: reps &lt;- replicate(1e3, {N &lt;- 50 x1 &lt;- runif(N) x2 &lt;- runif(N) x3 &lt;- runif(N) y &lt;- x1 + rnorm(1)*x2 - rnorm(1)*x3 + rnorm(1)*x2*x3 + rnorm(1) + rnorm(N) m1 &lt;- lm(y~x1+x2+x3+x2:x3) data.frame(R2=summary(m1)$r.squared,r2=cor(y,predict(m1))^2) },simplify=FALSE) df &lt;- do.call(rbind,reps) max(abs(df$R2-df$r2)) # biggest absolute difference ## [1] 6.106227e-16 plot(df$R2,df$r2) 4.7 \\(R^2\\) value of more general models Suppose we have results from a general model that returns predictions \\(\\hat{y_{*i}}\\) for data points with a true value \\(y_{*i}\\). The \\(R_*^2\\) value for such a model would be: \\[\\begin{equation} R_*^2 = 1 - \\frac{\\sum_i (\\hat{y_{*i}}-y_{*i})^2}{\\sum_i (\\overline{y_*} - y_{*i})^2} \\end{equation}\\] Now, we can not easily rewrite this as a correlation, because we do not have a closed expression for the predicted values. Let’s assume some made up data as well as some arbitrary predictions: ytrue &lt;- c(5,8,3,1,0) ypred &lt;- c(4,9,2,0,1) For these values we can calculate the \\(R^2\\) value as: 1 - sum((ytrue-ypred)^2)/sum((ytrue-mean(ytrue))^2) ## [1] 0.8786408 However, the square of the correlation between the two is: cor(ytrue,ypred)^2 ## [1] 0.9082639 Which is not the same! Bibliography Whitlock, Michael, and Dolph Schluter. 2015. The Analysis of Biological Data. Vol. 768. Roberts Publishers Greenwood Village, Colorado. "],["survival-analysis.html", "5 Survival analysis 5.1 Proportional hazard models", " 5 Survival analysis 5.1 Proportional hazard models In proportional hazard models, we assume that the hazard for one category is proportional to the hazard in another. The hazard (\\(h(t)\\)) here is basically the chance of dying for an individual. If \\(S(t)\\) is the survival function, it is equal to: \\[\\begin{equation} h(t) = -\\frac{S&#39;(t)}{S(t)} \\end{equation}\\] which is the same as: \\[\\begin{equation} h(t) = -\\frac{\\text{d}\\text{log}(S(t)}{\\text{d}t} \\end{equation}\\] We can integrate both sides: \\[\\begin{equation} \\int_0^T h(t)\\text{d}t = -\\int_0^T \\frac{\\text{d}\\text{log}(S(t)}{\\text{d}t}\\text{d}t \\end{equation}\\] By realizing that \\(S(0)=1\\), this can be rearranged to be: \\[\\begin{equation} S(T) = e^{-\\int_0^T h(t)dt} \\end{equation}\\] What does this mean? Firstly, let’s say there is no hazard: \\[\\begin{equation} S(T) = e^{-\\int_0^T 0 dt} = 1, \\end{equation}\\] then everyone always survives. Let’s assume there’s a constant hazard, C: \\[\\begin{equation} S(T) = e^{-\\int_0^T C dt} = e^{-C\\cdot T}, \\end{equation}\\] now the surviving fraction declines exponentially. Under the proportional hazards model, the assumption is that the hazard function of one category is proportional to the hazard in another, e.g.: \\[\\begin{equation} h_2(t) = b\\cdot h_1(t). \\end{equation}\\] If we assume the hazard rate to be constant and set: \\[\\begin{equation} h_1(t) = C, \\end{equation}\\] we can see how this plays out in the survival rate: \\[\\begin{align} S_1(t) &amp;= e^{-C\\cdot t}\\\\ S_2(t) &amp;= e^{-b\\cdot C \\cdot t} = \\left(e^{-C \\cdot t}\\right)^b \\end{align}\\] "],["when-not-to-use-a-pca.html", "6 When not to use a PCA", " 6 When not to use a PCA The following is a brief example of when using a PCA to replace variables in your dataset might not be ideal. Firstly, let’s create some dummy explanatory data. We will generate four predictors, with negative correlations between the first and second and between the third and fourth: set.seed(3) library(mvtnorm) x &lt;- rmvnorm(50,mean=rep(0,4), sigma=matrix(c(1,-0.8,0,0, -0.8,1,0,0, 0,0,1,-0.8, 0,0,-0.8,1), nrow=4,byrow=TRUE)) When we now try to calculate the principal components, we find; pcs &lt;- prcomp(x) summary(pcs) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.5386 1.2615 0.41082 0.38645 ## Proportion of Variance 0.5535 0.3721 0.03946 0.03492 ## Cumulative Proportion 0.5535 0.9256 0.96508 1.00000 Hence, the first two axes together already explain over 90% of the data. We could thus decide in a predictive model to only use these two axes. newx &lt;- predict(pcs) Let’s next make a response variable, which is equal to the sum of the 4 explanatory variable with some added noise on top: y &lt;- apply(x,1,sum) + rnorm(nrow(x),0,0.5) d &lt;- as.data.frame(cbind(y=y,newx)) head(d) ## y PC1 PC2 PC3 PC4 ## 1 -1.6267028 -1.5775951 0.2800516 -0.3727663 0.35502422 ## 2 0.7772536 0.5864081 -0.8836272 0.3464420 0.04036141 ## 3 -0.2745109 -2.1333463 -1.5687629 -0.7284356 -0.21787624 ## 4 -1.0983262 -1.1603281 -0.5778170 -0.1408519 0.13538958 ## 5 -0.3078073 -1.0937142 0.1890704 0.3171644 0.64608988 ## 6 -0.7859098 -0.8988454 1.0151972 -0.6220823 0.40436840 We can use dredge (should we?) to compare all models: library(MuMIn) ## Warning: package &#39;MuMIn&#39; was built under R version 4.4.3 m.1 &lt;- lm(y~.,data=d,na.action=&quot;na.fail&quot;) modSel &lt;- as.data.frame(dredge(m.1)) ## Fixed term is &quot;(Intercept)&quot; options(digits=3) #best models head(modSel,3) ## (Intercept) PC1 PC2 PC3 PC4 df logLik AICc delta weight ## 13 0.0899 NA NA 1.74 -1.14 4 -40.3 89.4 0.000 0.355 ## 15 0.0899 NA -0.0816 1.74 -1.14 5 -39.4 90.1 0.670 0.254 ## 14 0.0899 -0.0634 NA 1.74 -1.14 5 -39.5 90.3 0.856 0.231 # worst models tail(modSel,3) ## (Intercept) PC1 PC2 PC3 PC4 df logLik AICc delta weight ## 3 0.0899 NA -0.0816 NA NA 3 -70.4 147 57.8 9.88e-14 ## 2 0.0899 -0.0634 NA NA NA 3 -70.4 147 57.9 9.62e-14 ## 4 0.0899 -0.0634 -0.0816 NA NA 4 -70.1 149 59.7 3.85e-14 This is an interesting finding: the model that performs best is the model that contains the two axes that explain the least amount of variation, while the model that explains worst is the model that contains the two axes that explain the most variation. We can look at this sligthly more indepth: m.12 &lt;- lm(y~PC1+PC2,data=d) summary(m.12) ## ## Call: ## lm(formula = y ~ PC1 + PC2, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1006 -0.6951 -0.0293 0.7041 2.0347 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0899 0.1435 0.63 0.53 ## PC1 -0.0634 0.0942 -0.67 0.50 ## PC2 -0.0816 0.1149 -0.71 0.48 ## ## Residual standard error: 1.01 on 47 degrees of freedom ## Multiple R-squared: 0.02, Adjusted R-squared: -0.0217 ## F-statistic: 0.479 on 2 and 47 DF, p-value: 0.622 m.34 &lt;- lm(y~PC3+PC4,data=d) summary(m.34) ## ## Call: ## lm(formula = y ~ PC3 + PC4, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1886 -0.3907 -0.0126 0.3651 1.2402 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0899 0.0790 1.14 0.26 ## PC3 1.7429 0.1942 8.98 9.3e-12 *** ## PC4 -1.1439 0.2064 -5.54 1.3e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.558 on 47 degrees of freedom ## Multiple R-squared: 0.703, Adjusted R-squared: 0.69 ## F-statistic: 55.6 on 2 and 47 DF, p-value: 4.05e-13 Hence, we might conclude that at least in this case using a PCA to reduce variables and then fit would not be ideal. That is not to say that it is never a useful approach, I just think that one should be cautious. "],["transforming-posterior-samples.html", "7 Transforming posterior samples 7.1 Binomial model 7.2 Beta zero one inflated model", " 7 Transforming posterior samples 7.1 Binomial model Let’s first try something relatively easy: a binomial model with one random effect. We first generate data: library(brms) set.seed(456) N &lt;- 50 Ni &lt;- 10 x &lt;- rnorm(N) id &lt;- sample(Ni,N,replace=TRUE) id.f &lt;- factor(id) id.vals &lt;- rnorm(Ni) lin.y &lt;- x + id.vals[id] p.y &lt;- plogis(lin.y) y &lt;- rbinom(N,1,p.y) d &lt;- data.frame(x=x,y=y,id=id.f) And then proceed with the analysis: # 1. A bernoulli model with random effect m1 &lt;- brm(y~x+(1|id),family=bernoulli, warmup=1000, iter=2000, data=d, backend=&quot;cmdstanr&quot;, file=&quot;BRMS_bernoulli&quot;) # data points for which we want predictions: newdat &lt;- data.frame(x=c(0.3,0.4),id=&#39;1&#39;) # predictions on linear scale m1.fit.lin &lt;- fitted(m1,summary=FALSE,newdata=newdat,scale = &#39;linear&#39;,re_formula=NA) ## Loading required package: rstan ## Warning: package &#39;rstan&#39; was built under R version 4.4.3 ## Loading required package: StanHeaders ## Warning: package &#39;StanHeaders&#39; was built under R version 4.4.3 ## ## rstan version 2.32.7 (Stan version 2.32.2) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions, ## change `threads_per_chain` option: ## rstan_options(threads_per_chain = 1) ## Do not specify &#39;-march=native&#39; in &#39;LOCAL_CPPFLAGS&#39; or a Makevars file # define a function for getting estimate, se and 95% CI sum.fun &lt;- function(x) c(mu=mean(x),sd=sd(x),quantile(x,probs=c(0.025,0.975))) # apply function for each row in newdat t(apply(plogis(m1.fit.lin),2,sum.fun)) ## mu sd 2.5% 97.5% ## [1,] 0.5661154 0.1563952 0.2212366 0.8396047 ## [2,] 0.5958201 0.1562524 0.2413899 0.8598464 # taking them directly from brms fitted(m1,summary=TRUE,newdata=newdat,scale = &#39;response&#39;,re_formula=NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.5661154 0.1563952 0.2212366 0.8396047 ## [2,] 0.5958201 0.1562524 0.2413899 0.8598464 Fortunately, we see that both methods give identical estimates. 7.2 Beta zero one inflated model We now move to a slightly more complicated model: a beta regression with zero-one inflation. Assume we have a 40% chance of having inflation, which in turn in 80% of the cases implies a 0 and in 20% of the cases a 1.The following code simply simulates some data for our model to fit on: set.seed(456) N &lt;- 50 Ni &lt;- 10 x &lt;- rnorm(N) id &lt;- sample(Ni,N,replace=TRUE) id.f &lt;- factor(id) id.vals &lt;- rnorm(Ni) lin.y &lt;- x + id.vals[id] p.y &lt;- plogis(lin.y) beta.var &lt;- 0.04 # convert according to wikipedia # maybe not the most professional source # but didn&#39;t have time to rederive right now # (nor to look up a better source) shape.1 &lt;- p.y*(p.y*(1-p.y)/beta.var -1) shape.2 &lt;- (1-p.y)*(p.y*(1-p.y)/beta.var -1) y.std &lt;- rbeta(N,1,shape.1,shape.2) # now add zero-one inflation: y &lt;- ifelse(runif(N)&lt;0.4,as.numeric(runif(N)&gt;0.8),y.std) d &lt;- data.frame(x=x,y=y,id=id.f) # not too involved plot plot(x,y) Now we move to the analysis, first we are naive and we simply try the logistic transformation of the predictions: # 2. A beta zero one inflated model with random effect m2 &lt;- brm(y~x+(1|id),family=zero_one_inflated_beta, warmup=1000, iter=2000, data=d, backend=&quot;cmdstanr&quot;, file=&quot;BRMS_beta_zero_one&quot;) # data points for which we want predictions: newdat &lt;- data.frame(x=c(0.3,0.4),id=&#39;1&#39;) # predictions on linear scale m2.fit.lin &lt;- fitted(m2,summary=FALSE,newdata=newdat,scale = &#39;linear&#39;,re_formula=NA) # define a function for getting estimate, se and 95% CI sum.fun &lt;- function(x) c(mu=mean(x),sd=sd(x),quantile(x,probs=c(0.025,0.975))) # apply function for each row in newdat t(apply(plogis(m2.fit.lin),2,sum.fun)) ## mu sd 2.5% 97.5% ## [1,] 0.5763428 0.08192093 0.4108652 0.7336134 ## [2,] 0.5653001 0.08322229 0.3990678 0.7266214 # taking them directly from brms fitted(m2,summary=TRUE,newdata=newdat,scale = &#39;response&#39;,re_formula=NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4103261 0.06361266 0.2858994 0.5397669 ## [2,] 0.4035142 0.06396549 0.2802933 0.5333942 Now, the two are different, simply because we have not applied the zero one inflation. Let us now try to extract these values: # extract chances of zero one inflation and conditional chance of one from the model: p.zoi &lt;- as_draws_array(m2,variable = c(&#39;zoi&#39;)) p.coi &lt;- as_draws_array(m2,variable = c(&#39;coi&#39;)) # define a function for getting estimate, se and 95% CI sum.fun.zoi &lt;- function(x){ x.corrected &lt;- x*(1-p.zoi) + p.zoi*p.coi c(mu=mean(x.corrected),sd=sd(x.corrected),quantile(x.corrected,probs=c(0.025,0.975))) } # apply function for each row in newdat t(apply(plogis(m2.fit.lin),2,sum.fun.zoi)) ## mu sd 2.5% 97.5% ## [1,] 0.4103261 0.06361266 0.2858994 0.5397669 ## [2,] 0.4035142 0.06396549 0.2802933 0.5333942 # taking them directly from brms fitted(m2,summary=TRUE,newdata=newdat,scale = &#39;response&#39;,re_formula=NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4103261 0.06361266 0.2858994 0.5397669 ## [2,] 0.4035142 0.06396549 0.2802933 0.5333942 And now they are identical indeed, as expected! "],["error-propagation.html", "8 Error propagation 8.1 Data simulation 8.2 Problem statement 8.3 Point estimates 8.4 Uncertainty using sampling 8.5 Uncertainty using delta method 8.6 Conclusion", " 8 Error propagation 8.1 Data simulation For this page, I will simulate some data. The data describes the number of visitors in one of two food trucks (A and B) over time. timepoints &lt;- 0:24 trucks &lt;- c(&#39;A&#39;,&#39;B&#39;) df &lt;- expand.grid(time=timepoints,truck=trucks) df$expected.visitors &lt;- with(df,ifelse(truck==&#39;A&#39;,exp(3-0.1*(time-12)^2),exp(3.2-0.07*(time-18)^2))) We now have expected values for each truck at each time point. library(ggplot2) ggplot(df,aes(x=time,y=expected.visitors,col=truck)) + geom_line() + scale_x_continuous(expand=c(0,0)) +theme_classic() From this we can simulate data, by drawing from a poisson distribution. The final data that we will work with, looks like this: df$visitors &lt;- rpois(nrow(df),df$expected.visitors) ggplot(df,aes(x=time,y=visitors,col=truck)) + geom_line() + scale_x_continuous(expand=c(0,0)) +theme_classic() 8.2 Problem statement We proceed with a relatively simple quadratic model fit, to describe the patterns in the data: m1 &lt;- glm(visitors~truck*time+truck*I(time^2),data=df,family=poisson) summary(m1) ## ## Call: ## glm(formula = visitors ~ truck * time + truck * I(time^2), family = poisson, ## data = df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.690334 2.113945 -6.003 1.94e-09 *** ## truckB -7.994173 3.601178 -2.220 0.0264 * ## time 2.632646 0.356181 7.391 1.45e-13 *** ## I(time^2) -0.110190 0.014791 -7.450 9.35e-14 *** ## truckB:time -0.007661 0.483004 -0.016 0.9873 ## truckB:I(time^2) 0.038036 0.017331 2.195 0.0282 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 540.32 on 49 degrees of freedom ## Residual deviance: 24.39 on 44 degrees of freedom ## AIC: 130.38 ## ## Number of Fisher Scoring iterations: 6 For this model, we are interested in the following: - What is the maximum number of visitors at each truck at any hour. - When does the maximum number of visitors arrive? - How are these quantities different for the two trucks - What is the uncertainty in each of the estimates 1–3. 8.3 Point estimates Our model prediction (\\(y\\)) for a specific truck \\(i\\) at a specific time \\(t\\) is (notation consistent with R): \\[\\begin{equation} y = e^{\\beta_{\\text{Intercept}} + \\beta_{\\text{truckB}} \\delta_{i,B} + \\beta_{\\text{time}}t + \\beta_{\\text{I(time^2)}}t^2 + \\beta_{\\text{truckB:time}}\\delta_{i,B}t + \\beta_{\\text{truckB:I(time^2)}}\\delta_{i,B}t^2} \\end{equation}\\] Here, \\(\\delta_{i,B}\\) is equal to 0 when we want to make an estimate for truck A (\\(i=A\\)) and 1 when we want to make an estimate for truck B (\\(i=B\\)). We now derive expressions for the maximum value and when this occurs. Note that because log transformations are monotonic, we can determine these points on the log scale. Or in other words, we can define a new quantity, \\(\\eta\\) which equals \\(\\log{y}\\): \\[\\begin{equation} \\eta = \\log{y} = {\\beta_{\\text{Intercept}} + \\beta_{\\text{truckB}} \\delta_{i,B} + \\beta_{\\text{time}}t + \\beta_{\\text{I(time^2)}}t^2 + \\beta_{\\text{truckB:time}}\\delta_{i,B}t + \\beta_{\\text{truckB:I(time^2)}}\\delta_{i,B}t^2} \\end{equation}\\] whenever \\(\\eta\\) is at it’s highest possible value, so is \\(y\\). This occurs when \\(\\frac{\\text{d}\\eta}{\\text{d}t}=0\\). Hence, when: \\[\\begin{equation} \\frac{\\text{d}\\eta}{\\text{d}t} = \\beta_{\\text{time}} + 2\\beta_{\\text{I(time^2)}}t + \\beta_{\\text{truckB:time}}\\delta_{i,B} + 2\\beta_{\\text{truckB:I(time^2)}}\\delta_{i,B}t=0 \\end{equation}\\] Which can be solved for \\(t\\) relatively easily: \\[\\begin{equation} t_\\text{max} = - \\frac{\\beta_{\\text{time}}+ \\beta_{\\text{truckB:time}}\\delta_{i,B}}{2(\\beta_{\\text{I(time^2)}} + \\beta_{\\text{truckB:I(time^2)}}\\delta_{i,B})} \\end{equation}\\] And for specific trucks, in this model we thus find: \\[\\begin{align} t_\\text{max,A} &amp;= - \\frac{\\beta_{\\text{time}}}{2\\beta_{\\text{I(time^2)}}}\\\\ t_\\text{max,B} &amp;= - \\frac{\\beta_{\\text{time}}+ \\beta_{\\text{truckB:time}}}{2(\\beta_{\\text{I(time^2)}} + \\beta_{\\text{truckB:I(time^2)}})} \\end{align}\\] The difference in maximum values is thus: \\[\\begin{equation} \\Delta t_\\text{max} = \\frac{\\beta_{\\text{time}}+ \\beta_{\\text{truckB:time}}}{2(\\beta_{\\text{I(time^2)}} + \\beta_{\\text{truckB:I(time^2)}})} - \\frac{\\beta_{\\text{time}}}{2\\beta_{\\text{I(time^2)}}} \\end{equation}\\] The corresponding maximum values of \\(\\eta\\) are then: \\[\\begin{align} \\eta_\\text{max,A} &amp;= \\beta_{\\text{Intercept}} + \\beta_{\\text{time}} t_\\text{max,A} + \\beta_{\\text{I(time^2)}}t_\\text{max,A}^2 \\\\ \\eta_\\text{max,B} &amp;= \\beta_{\\text{Intercept}} + \\beta_{\\text{truckB}} + (\\beta_{\\text{time}}+\\beta_{\\text{truckB:time}})t_\\text{max,B} + (\\beta_{\\text{I(time^2)}}+\\beta_{\\text{truckB:I(time^2)}})t_\\text{max,B}^2 \\end{align}\\] And the difference is thus: \\[\\begin{equation} \\Delta \\eta_\\text{max} = \\beta_{\\text{time}} (t_\\text{max,A} - t_\\text{max,B})+ \\beta_{\\text{I(time^2)}}(t_\\text{max,A}^2 - t_\\text{max,B}^2) - \\beta_{\\text{truckB:time}}t_\\text{max,B} - \\beta_{\\text{truckB:I(time^2)}}t_\\text{max,B}^2 \\end{equation}\\] For the specific model fit, we end up with (for the time points): tmax.a &lt;- -coef(m1)[3]/(2*coef(m1)[4]) tmax.b &lt;- -(coef(m1)[3]+coef(m1)[5])/(2*(coef(m1)[4]+coef(m1)[6])) Delta.time &lt;- tmax.a-tmax.b tmax.a ## time ## 11.94595 tmax.b ## time ## 18.19023 tmax.a-tmax.b ## time ## -6.244285 These values are close to the time points at which the maximum occurs in the two graphs! The difference between them is negative due to the maximum occurring later for truck B. The corresponding maxima are then (on the log scale): eta.max.a &lt;- coef(m1) %*% c(1,0,tmax.a,tmax.a^2,0,0) eta.max.b &lt;- coef(m1) %*% c(1,1,tmax.b,tmax.b^2,tmax.b,tmax.b^2) delta.eta.max &lt;- eta.max.a - eta.max.b And on the response scale, that becomes: exp(eta.max.a) ## [,1] ## [1,] 20.7883 exp(eta.max.b) ## [,1] ## [1,] 24.28934 exp(delta.eta.max) ## [,1] ## [1,] 0.8558611 Note here that the last number corresponds to the ratio between the first two. 8.4 Uncertainty using sampling Probably the easiest way to get some view of the uncertainty, is by sampling parameters. library(mvtnorm) samples &lt;- rmvnorm(1e5,coef(m1),vcov(m1)) # we draw 1e6 different parameter combinations # for each combination we calculate tmax values: tmax.as &lt;- -samples[,3]/(2*samples[,4]) tmax.bs &lt;- -(samples[,3]+samples[,5])/(2*(samples[,4]+samples[,6])) delta.tmaxs &lt;- tmax.as - tmax.bs # confints: quantile(tmax.as,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 11.53233 12.35906 quantile(tmax.bs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 17.76564 18.64343 quantile(delta.tmaxs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## -6.853705 -5.654158 # we can do the same for eta max: eta.max.as &lt;- samples[,1] + samples[,3] * tmax.as + samples[,4] * tmax.as^2 eta.max.bs &lt;- samples[,1] + samples[,2] + (samples[,3]+samples[,5]) * tmax.bs + (samples[,4]+samples[,6]) * tmax.bs^2 delta.maxs &lt;- eta.max.as - eta.max.bs # confints: quantile(eta.max.as,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 2.811468 3.266305 quantile(eta.max.bs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 2.998823 3.387557 quantile(delta.maxs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## -0.4534911 0.1446363 # on the response scale: quantile(exp(eta.max.as),probs=c(0.025,0.975)) ## 2.5% 97.5% ## 16.63433 26.21430 quantile(exp(eta.max.bs),probs=c(0.025,0.975)) ## 2.5% 97.5% ## 20.06191 29.59356 quantile(exp(delta.maxs),probs=c(0.025,0.975)) ## 2.5% 97.5% ## 0.635406 1.155619 8.5 Uncertainty using delta method Now, we will approximate the 95% CIs using the delta method. This method is very nicely described by Cooch and White (2024) . Recall the expression for the time at which the maximum occurs for truck A: \\[\\begin{equation} \\hat{t_\\text{max,A}} = - \\frac{\\beta_{\\text{time}}}{2\\beta_{\\text{I(time^2)}}} \\end{equation}\\] For the delta method, we need to find the set of partial derivatives of \\(\\hat{t_\\text{max,A}}\\) with respect to each of the model parameters: \\[\\begin{align} \\frac{\\partial \\hat{t_\\text{max,A}}}{\\partial \\beta_{\\text{Intercept}}} &amp;= 0\\\\ \\frac{\\partial \\hat{t_\\text{max,A}}}{\\partial \\beta_{\\text{truckB}}} &amp;= 0\\\\ \\frac{\\partial \\hat{t_\\text{max,A}}}{\\partial \\beta_{\\text{time}}} &amp;= - \\frac{1}{2\\beta_{\\text{I(time^2)}}}\\\\ \\frac{\\partial \\hat{t_\\text{max,A}}}{\\partial \\beta_{\\text{I(time^2)}}} &amp;= \\frac{\\beta_{\\text{time}}}{2\\beta_{\\text{I(time^2)}}^2}\\\\ \\frac{\\partial \\hat{t_\\text{max,A}}}{\\partial \\beta_{\\text{truckB:time}}} &amp;= 0\\\\ \\frac{\\partial \\hat{t_\\text{max,A}}}{\\partial \\beta_{\\text{truckB:I(time^2)}}} &amp;= 0\\\\ \\end{align}\\] In slightly shorter notation, we can write: \\[\\begin{equation} \\nabla \\hat{t_\\text{max,A}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ - \\frac{1}{2\\beta_{\\text{I(time^2)}}} \\\\ \\frac{\\beta_{\\text{time}}}{2\\beta_{\\text{I(time^2)}}^2} \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{equation}\\] We can get to the uncertainty using this expression together with the variance covariance matrix (\\(\\Sigma\\)): \\[\\begin{equation} \\text{Var}(\\hat{t_\\text{max,A}}) = \\nabla \\hat{t_\\text{max,A}}^\\text{T} \\cdot \\Sigma \\cdot \\nabla \\hat{t_\\text{max,A}} \\end{equation}\\] We can take the square root of this variance to obtain the standard error, and then approximate the 95% CI using this standard error. In R notation: vcv &lt;- vcov(m1) # get vcv derivs &lt;- c(0,0,-1/(2*coef(m1)[4]), coef(m1)[3]/(2*coef(m1)[4]^2),0,0) # gradient # uncertainty (variance): var.tmax.a &lt;- as.numeric(derivs%*% vcv %*% derivs) se.tmax.a &lt;- sqrt(var.tmax.a) # 95% CI: tmax.a.CI &lt;- tmax.a + qnorm(c(0.025,0.975))*se.tmax.a tmax.a.CI ## [1] 11.54967 12.34222 # compare to the sampling 95% CI: quantile(tmax.as,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 11.53233 12.35906 We can calculate the CI for \\(t_\\text{max,B}\\) anaogously: \\[\\begin{equation} \\nabla \\hat{t_\\text{max,B}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ - \\frac{1}{2(\\beta_{\\text{I(time^2)}}+\\beta_{\\text{truckB:I(time^2)}})} \\\\ \\frac{\\beta_{\\text{time}}+\\beta_{\\text{truckB:time}}}{2(\\beta_{\\text{I(time^2)}}+\\beta_{\\text{truckB:I(time^2)}})^2} \\\\ - \\frac{1}{2(\\beta_{\\text{I(time^2)}}+\\beta_{\\text{truckB:I(time^2)}})} \\\\ \\frac{\\beta_{\\text{time}}+\\beta_{\\text{truckB:time}}}{2(\\beta_{\\text{I(time^2)}}+\\beta_{\\text{truckB:I(time^2)}})^2} \\end{bmatrix} \\end{equation}\\] and: \\[\\begin{equation} \\text{Var}(\\hat{t_\\text{max,B}}) = \\nabla \\hat{t_\\text{max,B}}^\\text{T} \\cdot \\Sigma \\cdot \\nabla \\hat{t_\\text{max,B}} \\end{equation}\\] From here it is then straightforward to calculate the standard error and 95% CI. vcv &lt;- vcov(m1) # get vcv derivs &lt;- c(0,0,-1/(2*(coef(m1)[4]+coef(m1)[6])), (coef(m1)[3]+coef(m1)[5])/(2*(coef(m1)[4]+coef(m1)[6])^2),-1/(2*(coef(m1)[4]+coef(m1)[6])),(coef(m1)[3]+coef(m1)[5])/(2*(coef(m1)[4]+coef(m1)[6])^2)) # gradient # uncertainty (variance): var.tmax.b &lt;- as.numeric(derivs%*% vcv %*% derivs) se.tmax.b &lt;- sqrt(var.tmax.b) # 95% CI: tmax.b.CI &lt;- tmax.b + qnorm(c(0.025,0.975))*se.tmax.b tmax.b.CI ## [1] 17.76630 18.61416 # compare to the sampling 95% CI: quantile(tmax.bs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 17.76564 18.64343 Next, we can do the same for the difference in time. We take a shortcut now, in the sense that we let R calculate the derivatives for us, using the Deriv library. library(Deriv) # we define a function that calculates delta t max: delta.t.max.fun &lt;- function(beta.0,beta.B,beta.time,beta.Bxtime,beta.time2,beta.Bxtime2){ (beta.time + beta.Bxtime)/(2*(beta.time2+beta.Bxtime2)) - beta.time/(2*beta.time2) } # below, for each of the coefficients, a function for the derivative is dermined and that function is evaluated: derivs &lt;- sapply(c(&quot;beta.0&quot;,&quot;beta.B&quot;,&quot;beta.time&quot;,&quot;beta.time2&quot;,&quot;beta.Bxtime&quot;,&quot;beta.Bxtime2&quot;), FUN = function(x){ derfun &lt;- Deriv(delta.t.max.fun,x) derfun(beta.0=coef(m1)[1],beta.B=coef(m1)[2], beta.time=coef(m1)[3],beta.time2=coef(m1)[4], beta.Bxtime=coef(m1)[5],beta.Bxtime2=coef(m1)[6]) }) Deriv(delta.t.max.fun,&quot;beta.time&quot;) ## function (beta.0, beta.B, beta.time, beta.Bxtime, beta.time2, ## beta.Bxtime2) ## 1/(2 * (beta.Bxtime2 + beta.time2)) - 1/(2 * beta.time2) # In a similar fashion as before, we can no proceed and calculate the 95% CI: # uncertainty (variance): var.delta.tmax &lt;- as.numeric(derivs%*% vcv %*% derivs) se.delta.tmax &lt;- sqrt(var.delta.tmax) # 95% CI: tmax.delta.CI &lt;- (tmax.a-tmax.b) + qnorm(c(0.025,0.975))*se.delta.tmax tmax.delta.CI ## [1] -6.824592 -5.663979 # compare to the sampling 95% CI: quantile(delta.tmaxs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## -6.853705 -5.654158 Again close enough! We proceed in a similar fashion for the maximum values, first defining appropriate functions and then using the machinery from above to calculate CIs: eta.max.a.fun &lt;- function(beta.0,beta.B,beta.time,beta.Bxtime,beta.time2,beta.Bxtime2){ tmax.a &lt;- - beta.time/(2*beta.time2) beta.0 + beta.time*tmax.a + beta.time2*tmax.a^2 } eta.max.b.fun &lt;- function(beta.0,beta.B,beta.time,beta.Bxtime,beta.time2,beta.Bxtime2){ tmax.b &lt;- - (beta.time+beta.Bxtime)/(2*(beta.time2+beta.Bxtime2)) beta.0 + beta.B + (beta.time+beta.Bxtime)*tmax.b + (beta.time2+beta.Bxtime2)*tmax.b^2 } delta.eta.max.fun &lt;- function(beta.0,beta.B,beta.time,beta.Bxtime,beta.time2,beta.Bxtime2){ tmax.a &lt;- - beta.time/(2*beta.time2) tmax.b &lt;- - (beta.time+beta.Bxtime)/(2*(beta.time2+beta.Bxtime2)) eta.max.a &lt;- beta.0 + beta.time*tmax.a + beta.time2*tmax.a^2 eta.max.b &lt;- beta.0 + beta.B + (beta.time+beta.Bxtime)*tmax.b + (beta.time2+beta.Bxtime2)*tmax.b^2 eta.max.a - eta.max.b } calc.CI &lt;- function(mod,func){ expval &lt;- func(beta.0=coef(mod)[1],beta.B=coef(mod)[2], beta.time=coef(mod)[3],beta.time2=coef(mod)[4], beta.Bxtime=coef(mod)[5],beta.Bxtime2=coef(mod)[6]) terms &lt;- c(&quot;beta.0&quot;,&quot;beta.B&quot;,&quot;beta.time&quot;,&quot;beta.time2&quot;,&quot;beta.Bxtime&quot;,&quot;beta.Bxtime2&quot;) derivs &lt;- rep(0,length(terms)) for(i in 1:length(terms)){ derfun &lt;- Deriv(func,terms[i]) derivs[i] &lt;- derfun(beta.0=coef(mod)[1],beta.B=coef(mod)[2], beta.time=coef(mod)[3],beta.time2=coef(mod)[4], beta.Bxtime=coef(mod)[5],beta.Bxtime2=coef(mod)[6]) } vcv &lt;- vcov(mod) var.val &lt;- as.numeric(derivs%*% vcv %*% derivs) se.val &lt;- sqrt(var.val) # 95% CI: expval + qnorm(c(0.025,0.975))*se.val } # CI for eta.max.a: calc.CI(m1,eta.max.a.fun) ## [1] 2.806549 3.262232 # compare to the sampling 95% CI: quantile(eta.max.as,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 2.811468 3.266305 # CI for eta.max.b: calc.CI(m1,eta.max.b.fun) ## [1] 2.996256 3.383819 # compare to the sampling 95% CI: quantile(eta.max.bs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 2.998823 3.387557 ## CI for the difference: # CI for eta.max.b: calc.CI(m1,delta.eta.max.fun) ## [1] -0.4547507 0.1434563 # compare to the sampling 95% CI: quantile(delta.maxs,probs=c(0.025,0.975)) ## 2.5% 97.5% ## -0.4534911 0.1446363 8.6 Conclusion The sampling and delta method gave near identical results. We conclude that for the fitted model, the maximum number of visitors at truck A occurs at 11.9 with a 95% CI of [11.55, 12.34] and for truck B at time 18.2 with a 95% CI of [17.77, 18.61]. The difference in timing of the maxima between the two trucks is -6.2 with a 95% CI of [-6.82, -5.66]. Here, negative numbers indicate that the maximum occurred first in truck A. The maximum values themselves on the log scale were for truck A 3.03 with a 95% CI of [2.81, 3.26] and for truck B 3.19 with a 95% CI of [3, 3.38]. The difference in between maxima between the two trucks is -0.16 with a 95% CI of [-0.45, 0.14]. Here, negative values would indicate that truck B has a higher maximum than truck A. On the observable scale (i.e. actual counts), the highest number of visitors at truck A was 20.79 with a 95% CI of [16.55, 26.11] and for truck B 24.29 with a 95% CI of [20.01, 29.48]. The relative difference between the maximum number of visitors between the two trucks was 0.86 with a 95% CI of [0.63, 1.15]. This means that the maximum number of visitors for truck A was 0.86 x the maximum number of visitors for truck B. Bibliography Cooch, E., and G. White, eds. 2024. “Appendix b – the ‘Delta Method’ ...” In Program MARK – a Gentle Introduction. http://www.phidot.org/software/mark/docs/book/. "],["proportion-of-negative-values-from-a-normal-distribution.html", "9 Proportion of negative values from a normal distribution 9.1 Invariability of the proportion of negative values 9.2 Adapting the mean to find a given proportion of negative values 9.3 Adapting the standard deviation 9.4 Simplified 9.5 Matching an empirical distribution", " 9 Proportion of negative values from a normal distribution 9.1 Invariability of the proportion of negative values For a given normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the proportion of negative values does not change when \\(\\mu\\) and \\(\\sigma\\) are multiplied by the same positive constant: pnorm(0,mean=5.3,sd=2.1) ## [1] 0.005804541 multi.factors &lt;- runif(100,0,100) # generate random multiplication factors prop.zeroes &lt;- pnorm(0,mean=multi.factors*5.3,sd=multi.factors*2.1) var(prop.zeroes) # no variation in proportion of zeroes! ## [1] 0 If we find one combination of mean and standard deviation that satisfies a certain fraction of negative values, we can thus generate infinitely many combinations that satisfy these conditions, simply by multiplying both mean and standard deviation with the same positive constant. 9.2 Adapting the mean to find a given proportion of negative values Assume we want to find a distribution with a certain proportion of zeroes, \\(p_0\\), and a set standard deviation \\(\\sigma\\). We can proceed as follows: Pick an arbitrary starting value for the mean \\(\\mu_0\\) Use to determine for the distribution with mean \\(\\mu_0\\) and standard deviation \\(\\sigma\\) at which cut-off value the fraction of random numbers below the cut-off is equal to \\(p_0\\). Shift the mean with that value, such that the cut-off aligns with the 0 point. You have found the distribution that you need! An example: assume we want to find the mean of a distribution for which \\(\\sigma=0.2\\) and a proportion of \\(p_0=0.21\\) of the random draws is negative: mu_0 &lt;- 5.67 # determine a random m_0 dmu &lt;- qnorm(0.21,mean=mu_0,sd=0.2) dmu # a fraction of 0.21 of the points is below this cut-off ## [1] 5.508716 mu &lt;- mu_0 - dmu # we shift the whole distribution pnorm(0,mu,0.2) # we found a distribution for which exactly 21% of the datapoints are below zero! ## [1] 0.21 # it has mean: mu ## [1] 0.1612842 # and standard deviation (as predefined) 0.2 ## [1] 0.2 9.3 Adapting the standard deviation We can adapt the above procedure to find the standard deviation \\(\\sigma\\) for which a certain proportion \\(p_0\\) of the data points is negative, given a predefined mean value of the distribution of \\(\\mu\\). Pick an arbitrary starting value for the mean \\(\\mu_0\\) and the standard deviation \\(\\sigma_0\\) Use to determine for the distribution with mean \\(\\mu_0\\) and standard deviation \\(\\sigma_0\\) at which cut-off value the fraction of random numbers below the cut-off is equal to \\(p_0\\). Shift the mean with that value, such that the cut-off aligns with the 0 point, this give \\(\\mu_0^\\prime\\). Multiply both \\(\\sigma_0\\) and \\(\\mu_0^\\prime\\) with \\(\\frac{mu}{\\mu_0^\\prime}\\) to find the distribution that you need. An example: assume we want to find the standard deviation of a distribution for which \\(\\mu=3.4\\) and a proportion of \\(p_0=0.17\\) of the random draws is negative: mu_0 &lt;- 2.45 # determine a random m_0 sd_0 &lt;- 1.14 p_0 &lt;- 0.17 dmu &lt;- qnorm(p_0,mean=mu_0,sd=sd_0) dmu # a fraction of 0.21 of the points is below this cut-off ## [1] 1.362252 mu_0p &lt;- mu_0 - dmu # we shift the whole distribution mu &lt;- 3.4 # the mean value that we would want to have mu &lt;- mu_0p * (mu/mu_0p)# Multiply the temporary values of mu and sd with a factor to match the predefined mean sd &lt;- sd_0 *(mu/mu_0p) pnorm(0,mu,sd) # we found a distribution for which exactly 17% of the datapoints are below zero! ## [1] 0.17 # it has mean (as predefined): mu ## [1] 3.4 # and standard deviation: sd ## [1] 3.563324 9.4 Simplified We can simplify the procedure in the previous section by starting with the standard normal distribution with \\(\\mu_0=0\\) and \\(\\sigma_0=1\\). Let’s say we want to find a proportion of \\(p_0=0.06\\) negative values with \\(\\mu=1.2\\): p_0 &lt;- 0.06 mu_0p &lt;- -qnorm(p_0,0,1) mu_0p # the mu necessary for p_0=0.06, when sigma=1 ## [1] 1.554774 # therefore also equal to the ratio mu/sigma such that p_0 = 0.06 mu &lt;- 1.2 # the mean value that we would want to have mu &lt;- mu_0p * (mu/mu_0p)# Multiply the temporary values of mu and sd with a factor to match the predefined mean sd &lt;- (mu/mu_0p) pnorm(0,mu,sd) # we found a distribution for which exactly 17% of the datapoints are below zero! ## [1] 0.06 # it has mean (as predefined): mu ## [1] 1.2 # and standard deviation: sd ## [1] 0.7718166 9.5 Matching an empirical distribution The means above are all based on the means of full distributions, including the negative values. How can one approximately match a distribution that was generated by drawing random numbers from a normal distribution and then setting all negative values to 0? If the proportion of negative values is smaller than 50%, one can obtain a rough approximation of the mean by removing the same percentage of high values from the data and calculate the mean on the remaining data. For example: # generate data from a normal distribution (1000 datpoints): x &lt;- rnorm(1000,mean=0.45,sd=0.85) # we want to find this mean and sd back x[x&lt;0] &lt;- 0 # all negative values are set to 0 sd(x) ## [1] 0.6267868 mean(x) ## [1] 0.6008023 # we note that the sd and mean of these values are way off Now that we have some dummy data, how can we find the mean and sd values back? # calculate the proportion of zereos: p_0 &lt;- mean(x==0) p_0 ## [1] 0.293 # Remove the same fraction of points on the high side # and calculate the mean for the remaining values q1 &lt;- quantile(x,probs=1-p_0) mu_approx &lt;- mean(x[x&gt;0 &amp; x &lt;q1]) # a pretty okay approximation! mu_approx ## [1] 0.4451164 # now we want to calculate the necessary sd such that a proportion p_0 for a distribution with mean mu_approx is below 0 multip &lt;- -qnorm(p_0) # the factor between mu and sd sd_approx &lt;- mu_approx/multip sd_approx # a very fair approximation of the standard deviation! ## [1] 0.8172647 There must be better ways to achieve this, if you know a good method, please let me know! "],["bibliography.html", "Bibliography", " Bibliography Cooch, E., and G. White, eds. 2024. “Appendix b – the ‘Delta Method’ ...” In Program MARK – a Gentle Introduction. http://www.phidot.org/software/mark/docs/book/. Whitlock, Michael, and Dolph Schluter. 2015. The Analysis of Biological Data. Vol. 768. Roberts Publishers Greenwood Village, Colorado. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
