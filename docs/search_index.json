[["index.html", "Stats notes Ramblings", " Stats notes Koen van Benthem 22-06-23 Ramblings "],["introduction.html", "1 Introduction", " 1 Introduction These pages serve as a location to store notes that I have made on statistics. Some of it might be reliable, some it might not. It is mainly intended for my own use, but if you stumble upon these pages and want to explore - feel free! "],["differences-between-two-groups.html", "2 Differences between two groups 2.1 Notation 2.2 t-test 2.3 ANOVA 2.4 lm", " 2 Differences between two groups Here, I try to show that ANOVA, two sample t-test and lm return the same result when the data only consist of two groups. The recipes for ANOVA, t-test and lm were taken from Whitlock and Schluter (2015) . 2.1 Notation Assume we have an explanatory variable \\(x\\) that was only measured at two specific values: \\(A\\) and \\(B\\) (e.g. \\(x\\) was temperature and only two different values, e.g. \\(A=18^\\circ C\\) and \\(B=21^\\circ C\\)). In order to simplify the notation, the datapoints have been ordered, such that the first \\(N_A\\) datapoints have \\(x=A\\) and the last \\(N_B\\) have \\(x=B\\), with a total number of data points \\(N=N_A+N_B\\). We assume the response variable \\(y\\) to be normally distributed. It may for example represent body size. We note that the group specific means of \\(y\\) are: \\[\\begin{equation} \\overline{y_A} = \\frac{1}{N_A}\\sum_{i=1}^{N_A} y_i \\end{equation}\\] \\[\\begin{equation} \\overline{y_B} = \\frac{1}{N_B}\\sum_{i=N_A+1}^{N} y_i \\end{equation}\\] and the overall mean: \\[\\begin{equation} \\overline{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i \\end{equation}\\] 2.2 t-test For a simple t-test, we first have to calculate the standard error, which in turn depends on the pooled sample variance, which in turn depends on the separate variances. We denote the variances for group A and B as: \\[\\begin{align} s_A^2 &amp;= \\sum_{i=1}^{N_A}\\frac{(y_i-\\overline{y_A})^2}{N_A-1} \\\\ s_B^2 &amp;= \\sum_{i=N_A+1}^{N_B}\\frac{(y_i-\\overline{y_B})^2}{N_B-1} \\end{align}\\] The pooled sample variance is then: \\[\\begin{equation} s_p^2 = \\frac{\\text{df}_As_A^2 + \\text{df}_Bs_B^2}{\\text{df}_A+\\text{df}_B} = \\frac{\\sum_{i=1}^{N_A}(N_A-1)\\frac{(y_i-\\overline{y_A})^2}{N_A-1} + \\sum_{i=N_A+1}^{N_B}(N_B-1)\\frac{(y_i-\\overline{y_B})^2}{N_B-1} }{N - 2} \\end{equation}\\] The standard error is then: \\[\\begin{equation} \\text{SE}_{\\overline{y_A}-\\overline{y_B}} = \\sqrt{s_p^2\\left(\\frac{1}{N_A}+\\frac{1}{N_B}\\right)} = \\sqrt{\\frac{\\sum_{i=1}^{N_A}(y_i-\\overline{y_A})^2 + \\sum_{i=N_A+1}^{N_B}(y_i-\\overline{y_B})^2 }{N - 2} \\left(\\frac{1}{N_A}+\\frac{1}{N_B}\\right)} \\end{equation}\\] the test statistic is: \\[\\begin{equation} t=\\frac{\\overline{y_A}-\\overline{y_B}}{\\text{SE}_{\\overline{y_A}-\\overline{y_B}}} = \\sqrt{\\frac{N_AN_B(N-2)(\\overline{y_A}-\\overline{y_B})^2}{N\\left(\\sum_{i=1}^{N_A}(y_i-\\overline{y_A})^2 + \\sum_{i=N_A+1}^{N_B}(y_i-\\overline{y_B})^2\\right)}} \\end{equation}\\] 2.3 ANOVA We first calculate the group sum of squares: \\[\\begin{equation} \\text{SS}_\\text{groups} = N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2 \\end{equation}\\] as well as the error sum of squares: \\[\\begin{equation} \\text{SS}_\\text{error} = \\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2 \\end{equation}\\] From here, we can easily obtain the \\(F\\)-statistic as: \\[\\begin{equation} F = \\frac{(N-2)\\left(N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2\\right)}{\\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2} \\end{equation}\\] This already looks a bit like \\(t\\), but in order to see the equivalency between the two, we have a bit more rewriting to do. Specifically, we will focus on: \\[\\begin{align} N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2 &amp;= N_A\\overline{y_A}^2 - 2N_A \\overline{y_A}\\overline{y} + N_A\\overline{y}^2 + N_B\\overline{y_B}^2 - 2N_B\\overline{y_B}\\overline{y} + N_B \\overline{y}^2 \\\\ &amp;= N_A\\overline{y_A}^2 - 2(N_A \\overline{y_A}+N_B\\overline{y_B})\\overline{y} + (N_A+N_B)\\overline{y}^2 + N_B\\overline{y_B}^2\\\\ &amp;=N_A\\overline{y_A}^2 - \\frac{(N_A \\overline{y_A}+N_B\\overline{y_B})^2}{N} + N_B\\overline{y_B}^2\\\\ &amp;= N_A\\overline{y_A}^2 - \\frac{N_A^2 \\overline{y_A}^2+N_B^2\\overline{y_B}^2 + 2N_AN_B\\overline{y_A}\\overline{y_B}}{N} + N_B\\overline{y_B}^2\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\frac{N}{N_B}\\overline{y_A}^2 - \\frac{N_A}{N_B} \\overline{y_A}^2-\\frac{N_B}{N_A}\\overline{y_B}^2 - 2\\overline{y_A}\\overline{y_B} + \\frac{N}{N_A}\\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\frac{N-N_A}{N_B}\\overline{y_A}^2 - 2\\overline{y_A}\\overline{y_B} + \\frac{N-N_B}{N_A}\\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\overline{y_A}^2 - 2\\overline{y_A}\\overline{y_B} + \\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\overline{y_A} - \\overline{y_B}\\right)^2\\\\ \\end{align}\\] Where in going from the second to the third line, we have used that \\(N\\overline{y}=N_A\\overline{y_A}+N_B\\overline{y_B}\\), and on going from the sixt to the seventh line that \\(N-N_A = N_B\\) and equivalently \\(N-N_B= N_A\\). We can not use this identity to rewrite our \\(F\\)-statistic: \\[\\begin{equation} F = \\frac{(N-2)N_AN_B\\left(\\overline{y_A}-\\overline{y_B}\\right)^2}{N\\left(\\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2\\right)} = t^2 \\end{equation}\\] and hence \\(F=t^2\\). 2.4 lm References "],["survival-analysis.html", "3 Survival analysis 3.1 Proportional hazard models", " 3 Survival analysis 3.1 Proportional hazard models In proportional hazard models, we assume that the hazard for one category is proportional to the hazard in another. The hazard (\\(h(t)\\)) here is basically the chance of dying for an individual. If \\(S(t)\\) is the survival function, it is equal to: \\[\\begin{equation} h(t) = -\\frac{S&#39;(t)}{S(t)} \\end{equation}\\] which is the same as: \\[\\begin{equation} h(t) = -\\frac{\\text{d}\\text{log}(S(t)}{\\text{d}t} \\end{equation}\\] We can integrate both sides: \\[\\begin{equation} \\int_0^T h(t)\\text{d}t = -\\int_0^T \\frac{\\text{d}\\text{log}(S(t)}{\\text{d}t}\\text{d}t \\end{equation}\\] By realizing that \\(S(0)=1\\), this can be rearranged to be: \\[\\begin{equation} S(T) = e^{-\\int_0^T h(t)dt} \\end{equation}\\] What does this mean? Firstly, let’s say there is no hazard: \\[\\begin{equation} S(T) = e^{-\\int_0^T 0 dt} = 1, \\end{equation}\\] then everyone always survives. Let’s assume there’s a constant hazard, C: \\[\\begin{equation} S(T) = e^{-\\int_0^T C dt} = e^{-C\\cdot T}, \\end{equation}\\] now the surviving fraction declines exponentially. Under the proportional hazards model, the assumption is that the hazard function of one category is proportional to the hazard in another, e.g.: \\[\\begin{equation} h_2(t) = b\\cdot h_1(t). \\end{equation}\\] If we assume the hazard rate to be constant and set: \\[\\begin{equation} h_1(t) = C, \\end{equation}\\] we can see how this plays out in the survival rate: \\[\\begin{align} S_1(t) &amp;= e^{-C\\cdot t}\\\\ S_2(t) &amp;= e^{-b\\cdot C \\cdot t} = \\left(e^{-C \\cdot t}\\right)^b \\end{align}\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
