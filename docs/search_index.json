[["index.html", "Stats notes Ramblings", " Stats notes Koen van Benthem 22-06-23 Ramblings "],["introduction.html", "1 Introduction", " 1 Introduction These pages serve as a location to store notes that I have made on statistics. Some of it might be reliable, some of it might not. It is mainly intended for my own use, but if you stumble upon these pages and want to explore - feel free! "],["differences-between-two-groups.html", "2 Differences between two groups 2.1 Notation 2.2 t-test 2.3 ANOVA 2.4 linear model", " 2 Differences between two groups Here, I try to show that ANOVA, two sample t-test and lm return the same result when the data only consist of two groups. The recipes for ANOVA, t-test and lm were taken from Whitlock and Schluter (2015) . 2.1 Notation Assume we have an explanatory variable \\(x\\) that was only measured at two specific values: \\(A\\) and \\(B\\) (e.g. \\(x\\) was temperature and only two different values, e.g. \\(A=18^\\circ C\\) and \\(B=21^\\circ C\\)). In order to simplify the notation, the datapoints have been ordered, such that the first \\(N_A\\) datapoints have \\(x=A\\) and the last \\(N_B\\) have \\(x=B\\), with a total number of data points \\(N=N_A+N_B\\). We assume the response variable \\(y\\) to be normally distributed. It may for example represent body size. We note that the group specific means of \\(y\\) are: \\[\\begin{equation} \\overline{y_A} = \\frac{1}{N_A}\\sum_{i=1}^{N_A} y_i \\end{equation}\\] \\[\\begin{equation} \\overline{y_B} = \\frac{1}{N_B}\\sum_{i=N_A+1}^{N} y_i \\end{equation}\\] and the overall mean: \\[\\begin{equation} \\overline{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i \\end{equation}\\] 2.2 t-test For a simple t-test, we first have to calculate the standard error, which in turn depends on the pooled sample variance, which in turn depends on the separate variances. We denote the variances for group A and B as: \\[\\begin{align} s_A^2 &amp;= \\sum_{i=1}^{N_A}\\frac{(y_i-\\overline{y_A})^2}{N_A-1} \\\\ s_B^2 &amp;= \\sum_{i=N_A+1}^{N_B}\\frac{(y_i-\\overline{y_B})^2}{N_B-1} \\end{align}\\] The pooled sample variance is then: \\[\\begin{equation} s_p^2 = \\frac{\\text{df}_As_A^2 + \\text{df}_Bs_B^2}{\\text{df}_A+\\text{df}_B} = \\frac{\\sum_{i=1}^{N_A}(N_A-1)\\frac{(y_i-\\overline{y_A})^2}{N_A-1} + \\sum_{i=N_A+1}^{N_B}(N_B-1)\\frac{(y_i-\\overline{y_B})^2}{N_B-1} }{N - 2} \\end{equation}\\] The standard error is then: \\[\\begin{equation} \\text{SE}_{\\overline{y_A}-\\overline{y_B}} = \\sqrt{s_p^2\\left(\\frac{1}{N_A}+\\frac{1}{N_B}\\right)} = \\sqrt{\\frac{\\sum_{i=1}^{N_A}(y_i-\\overline{y_A})^2 + \\sum_{i=N_A+1}^{N_B}(y_i-\\overline{y_B})^2 }{N - 2} \\left(\\frac{1}{N_A}+\\frac{1}{N_B}\\right)} \\end{equation}\\] the test statistic is: \\[\\begin{equation} t=\\frac{\\overline{y_A}-\\overline{y_B}}{\\text{SE}_{\\overline{y_A}-\\overline{y_B}}} = \\sqrt{\\frac{N_AN_B(N-2)(\\overline{y_A}-\\overline{y_B})^2}{N\\left(\\sum_{i=1}^{N_A}(y_i-\\overline{y_A})^2 + \\sum_{i=N_A+1}^{N_B}(y_i-\\overline{y_B})^2\\right)}} \\end{equation}\\] 2.3 ANOVA We first calculate the group sum of squares: \\[\\begin{equation} \\text{SS}_\\text{groups} = N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2 \\end{equation}\\] as well as the error sum of squares: \\[\\begin{equation} \\text{SS}_\\text{error} = \\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2 \\end{equation}\\] From here, we can easily obtain the \\(F\\)-statistic as: \\[\\begin{equation} F = \\frac{(N-2)\\left(N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2\\right)}{\\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2} \\end{equation}\\] This already looks a bit like \\(t\\), but in order to see the equivalency between the two, we have a bit more rewriting to do. Specifically, we will focus on: \\[\\begin{align} N_A (\\overline{y_A}-\\overline{y})^2 + N_B (\\overline{y_B}-\\overline{y})^2 &amp;= N_A\\overline{y_A}^2 - 2N_A \\overline{y_A}\\overline{y} + N_A\\overline{y}^2 + N_B\\overline{y_B}^2 - 2N_B\\overline{y_B}\\overline{y} + N_B \\overline{y}^2 \\\\ &amp;= N_A\\overline{y_A}^2 - 2(N_A \\overline{y_A}+N_B\\overline{y_B})\\overline{y} + (N_A+N_B)\\overline{y}^2 + N_B\\overline{y_B}^2\\\\ &amp;=N_A\\overline{y_A}^2 - \\frac{(N_A \\overline{y_A}+N_B\\overline{y_B})^2}{N} + N_B\\overline{y_B}^2\\\\ &amp;= N_A\\overline{y_A}^2 - \\frac{N_A^2 \\overline{y_A}^2+N_B^2\\overline{y_B}^2 + 2N_AN_B\\overline{y_A}\\overline{y_B}}{N} + N_B\\overline{y_B}^2\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\frac{N}{N_B}\\overline{y_A}^2 - \\frac{N_A}{N_B} \\overline{y_A}^2-\\frac{N_B}{N_A}\\overline{y_B}^2 - 2\\overline{y_A}\\overline{y_B} + \\frac{N}{N_A}\\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\frac{N-N_A}{N_B}\\overline{y_A}^2 - 2\\overline{y_A}\\overline{y_B} + \\frac{N-N_B}{N_A}\\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\overline{y_A}^2 - 2\\overline{y_A}\\overline{y_B} + \\overline{y_B}^2\\right)\\\\ &amp;= \\frac{N_AN_B}{N}\\left(\\overline{y_A} - \\overline{y_B}\\right)^2\\\\ \\end{align}\\] Where in going from the second to the third line, we have used that \\(N\\overline{y}=N_A\\overline{y_A}+N_B\\overline{y_B}\\), and on going from the sixt to the seventh line that \\(N-N_A = N_B\\) and equivalently \\(N-N_B= N_A\\). We can not use this identity to rewrite our \\(F\\)-statistic: \\[\\begin{equation} F = \\frac{(N-2)N_AN_B\\left(\\overline{y_A}-\\overline{y_B}\\right)^2}{N\\left(\\sum_{i=1}^{N_A} (y_i - \\overline{y_A})^2 + \\sum_{i=N_A+1}^{N} (y_i - \\overline{y_B})^2\\right)} = t^2 \\end{equation}\\] and hence \\(F=t^2\\). 2.4 linear model Using the formula in Whitlock and Schluter (2015) , we can determine the slope for the line between the points as: \\[\\begin{equation} b=\\frac{\\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N (x_i - \\overline{x})^2} \\end{equation}\\] with standard error: \\[\\begin{equation} \\text{SE}_b = \\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-b\\sum_{i=1}^N (x_i-\\overline{x})(y_i - \\overline{y})}{(N-2)\\sum_{i=1}^N (x_i-\\overline{x})^2}} \\end{equation}\\] In order to simplify this for the specific case with only two possible values of x, we first focus on the following simpler expressions: \\[\\begin{align} \\overline{x} &amp;= \\frac{N_A A + N_B B}{N} \\end{align}\\] We can use this to write: \\[\\begin{align} \\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y}) &amp;= \\sum_{i=1}^{N_A} (A - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) + \\sum_{i=N_A+1}^{N} (B - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) \\\\ &amp;= \\sum_{i=1}^{N_A} (\\frac{NA}{N} - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) + \\sum_{i=N_A+1}^{N} (\\frac{NB}{N} - \\frac{N_A A + N_B B}{N})(y_i - \\overline{y}) \\\\ &amp;= \\frac{1}{N}\\left(\\sum_{i=1}^{N_A} N_B (A - B)(y_i - \\overline{y}) + \\sum_{i=N_A+1}^{N} N_A (B - A)(y_i - \\overline{y})\\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(\\sum_{i=1}^{N_A} N_B (y_i - \\overline{y}) - \\sum_{i=N_A+1}^{N} N_A (y_i - \\overline{y})\\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(N_B \\sum_{i=1}^{N_A} y_i - N_BN_A\\overline{y} - N_A \\sum_{i=N_A+1}^{N} y_i + N_AN_B \\overline{y}\\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(N_B \\sum_{i=1}^{N_A} y_i - N_A \\sum_{i=N_A+1}^{N} y_i \\right) \\\\ &amp;= \\frac{(A - B)}{N}\\left(N_BN_A \\overline{y_A} - N_A N_B \\overline{y_B}\\right) \\\\ &amp;= \\frac{(A - B)N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right) \\\\ \\end{align}\\] Similarly, we attempt to simplify the term: \\[\\begin{align} \\sum_{i=1}^N (x_i - \\overline{x})^2 &amp;= \\sum_{i=1}^N (x_i - \\frac{N_A A + N_B B}{N})^2 \\\\ &amp;= \\sum_{i=1}^{N_A} (A - \\frac{N_A A + N_B B}{N})^2 + \\sum_{i=N_A+1}^N (B - \\frac{N_A A + N_B B}{N})^2 \\\\ &amp;= \\sum_{i=1}^{N_A} (\\frac{NA}{N} - \\frac{N_A A + N_B B}{N})^2 + \\sum_{i=N_A+1}^N (\\frac{NB}{N} - \\frac{N_A A + N_B B}{N})^2 \\\\ &amp;= \\sum_{i=1}^{N_A} (\\frac{N_B (A - B)}{N})^2 + \\sum_{i=N_A+1}^N (\\frac{N_A (B-A)}{N})^2 \\\\ &amp;= N_A (\\frac{N_B (A - B)}{N})^2 + N_B (\\frac{N_A (B-A)}{N})^2 \\\\ &amp;= (A-B)^2\\left(N_A \\frac{N_B^2 }{N^2} + N_B \\frac{N_A^2 }{N^2}\\right) \\\\ &amp;= (A-B)^2\\left( \\frac{N_AN_B^2 + N_B N_A^2 }{(N_A+N_B)^2}\\right) \\\\ &amp;= (A-B)^2\\left( \\frac{N_AN_B (N_A + N_B)}{(N_A+N_B)^2}\\right) \\\\ &amp;= (A-B)^2 \\frac{N_AN_B }{N} \\\\ \\end{align}\\] After a lot of rewriting, we can now use these expressions to find a much expected answer: \\[\\begin{align} b&amp;=\\frac{\\sum_{i=1}^N (x_i-\\overline{x})(y_i-\\overline{y})}{\\sum_{i=1}^N (x_i - \\overline{x})^2}\\\\ &amp;=\\frac{\\overline{y_A}-\\overline{y_B}}{A-B} \\end{align}\\] in other words: when we have only two distinct values for the \\(x\\)-variable, the slope is equal to the difference in the means in \\(y\\) between these to groups and the difference between the \\(x\\)-values. This result was of course completely to be expected, and we could have obtained it through reasoning alone, without the need to rewrite these equations at length. The next step is to rewrite the standard error for the slope: \\[\\begin{align} \\text{SE}_b &amp;= \\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-b\\sum_{i=1}^N (x_i-\\overline{x})(y_i - \\overline{y})}{(N-2)\\sum_{i=1}^N (x_i-\\overline{x})^2}} \\\\ &amp;= \\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-\\frac{\\overline{y_A}-\\overline{y_B}}{A-B}\\frac{(A - B)N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right)}{(N-2)(A-B)^2 \\frac{N_AN_B}{N}}} \\\\ &amp;= \\frac{1}{A-B}\\sqrt{\\frac{\\sum_{i=1}^N (y_i-\\overline{y})^2-\\frac{N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right)^2}{(N-2) \\frac{N_AN_B}{N}}} \\end{align}\\] Now, the test statistic for testing whether \\(b\\) is equal to zero is (\\(t_b\\)): \\[\\begin{align} t_b &amp;= \\frac{b}{\\text{SE}_b}\\\\ &amp;= \\sqrt{\\frac{(N-2) \\frac{N_AN_B}{N}(\\overline{y_A}-\\overline{y_B})^2}{\\sum_{i=1}^N (y_i-\\overline{y})^2-\\frac{N_AN_B}{N}\\left( \\overline{y_A} - \\overline{y_B}\\right)^2}} \\\\ &amp;= \\sqrt{\\frac{(N-2) N_AN_B(\\overline{y_A}-\\overline{y_B})^2}{N\\sum_{i=1}^N (y_i-\\overline{y})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2}} \\end{align}\\] Let’s first rewrite the following: \\[\\begin{align} N&amp;\\sum_{i=1}^N (y_i-\\overline{y})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2 = N\\sum_{i=1}^N (y_i^2-2y_i\\overline{y} + \\overline{y}^2)-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^N y_i^2-2N^2\\overline{y}^2 + N^2\\overline{y}^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^N y_i^2-(N\\overline{y})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^{N} y_i^2 -(N_A\\overline{y_A}+N_B\\overline{y_B})^2-N_AN_B\\left( \\overline{y_A} - \\overline{y_B}\\right)^2\\\\ &amp;= N\\sum_{i=1}^{N} y_i^2 -N_A^2\\overline{y_A}^2-N_B^2\\overline{y_B}^2-2N_AN_B\\overline{y_A}\\overline{y_B}-N_AN_B\\overline{y_A}^2 - N_AN_B\\overline{y_B}^2 + 2N_AN_B\\overline{y_A}\\overline{y_B}\\\\ &amp;= N\\sum_{i=1}^{N} y_i^2 -N_A(N_A+N_B)\\overline{y_A}^2-N_B(N_B+N_A)\\overline{y_B}^2 \\\\ &amp;= N\\left(\\sum_{i=1}^{N} y_i^2 -N_A\\overline{y_A}^2-N_B\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N} y_i^2 -2N_A\\overline{y_A}^2+N_A\\overline{y_A}^2-2N_B\\overline{y_B}^2+N_B\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N} y_i^2 -\\sum_{i=1}^{N_A} 2\\overline{y_A}y_i+\\sum_{i=1}^{N_A}\\overline{y_A}^2-2\\sum_{i=N_A+1}^N\\overline{y_B}y_i+\\sum_{i=N_B+1}^N\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N_A} y_i^2 + \\sum_{i=N_A+1}^{N} y_i^2 -\\sum_{i=1}^{N_A} 2\\overline{y_A}y_i+\\sum_{i=1}^{N_A}\\overline{y_A}^2-2\\sum_{i=N_A+1}^N\\overline{y_B}y_i+\\sum_{i=N_B+1}^N\\overline{y_B}^2\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N_A} \\left(y_i^2 - 2\\overline{y_A}y_i+ \\overline{y_A}^2 \\right) + \\sum_{i=N_A+1}^{N} \\left(y_i^2 -2\\overline{y_B}y_i+\\overline{y_B}^2 \\right)\\right) \\\\ &amp;= N\\left(\\sum_{i=1}^{N_A} \\left(y_i - \\overline{y_A} \\right)^2 + \\sum_{i=N_A+1}^{N} \\left(y_i -\\overline{y_B}\\right)^2\\right) \\\\ \\end{align}\\] By using this simplification, we can rewrite the value of the \\(t\\)-statistic as: \\[\\begin{equation} t_b = \\sqrt{\\frac{(N-2) N_AN_B(\\overline{y_A}-\\overline{y_B})^2}{ N\\left(\\sum_{i=1}^{N_A} \\left(y_i - \\overline{y_A} \\right)^2 + \\sum_{i=N_A+1}^{N} \\left(y_i -\\overline{y_B}\\right)^2\\right)}} = t = \\sqrt{F} \\end{equation}\\] and hence we conclude that \\(t=t_b\\). Bibliography "],["survival-analysis.html", "3 Survival analysis 3.1 Proportional hazard models", " 3 Survival analysis 3.1 Proportional hazard models In proportional hazard models, we assume that the hazard for one category is proportional to the hazard in another. The hazard (\\(h(t)\\)) here is basically the chance of dying for an individual. If \\(S(t)\\) is the survival function, it is equal to: \\[\\begin{equation} h(t) = -\\frac{S&#39;(t)}{S(t)} \\end{equation}\\] which is the same as: \\[\\begin{equation} h(t) = -\\frac{\\text{d}\\text{log}(S(t)}{\\text{d}t} \\end{equation}\\] We can integrate both sides: \\[\\begin{equation} \\int_0^T h(t)\\text{d}t = -\\int_0^T \\frac{\\text{d}\\text{log}(S(t)}{\\text{d}t}\\text{d}t \\end{equation}\\] By realizing that \\(S(0)=1\\), this can be rearranged to be: \\[\\begin{equation} S(T) = e^{-\\int_0^T h(t)dt} \\end{equation}\\] What does this mean? Firstly, let’s say there is no hazard: \\[\\begin{equation} S(T) = e^{-\\int_0^T 0 dt} = 1, \\end{equation}\\] then everyone always survives. Let’s assume there’s a constant hazard, C: \\[\\begin{equation} S(T) = e^{-\\int_0^T C dt} = e^{-C\\cdot T}, \\end{equation}\\] now the surviving fraction declines exponentially. Under the proportional hazards model, the assumption is that the hazard function of one category is proportional to the hazard in another, e.g.: \\[\\begin{equation} h_2(t) = b\\cdot h_1(t). \\end{equation}\\] If we assume the hazard rate to be constant and set: \\[\\begin{equation} h_1(t) = C, \\end{equation}\\] we can see how this plays out in the survival rate: \\[\\begin{align} S_1(t) &amp;= e^{-C\\cdot t}\\\\ S_2(t) &amp;= e^{-b\\cdot C \\cdot t} = \\left(e^{-C \\cdot t}\\right)^b \\end{align}\\] "],["when-not-to-use-a-pca.html", "4 When not to use a PCA", " 4 When not to use a PCA The following is a brief example of when using a PCA to replace variables in your dataset might not be ideal. Firstly, let’s create some dummy explanatory data. We will generate four predictors, with negative correlations between the first and second and between the third and fourth: set.seed(3) library(mvtnorm) x &lt;- rmvnorm(50,mean=rep(0,4), sigma=matrix(c(1,-0.8,0,0, -0.8,1,0,0, 0,0,1,-0.8, 0,0,-0.8,1), nrow=4,byrow=TRUE)) When we now try to calculate the principal components, we find; pcs &lt;- prcomp(x) summary(pcs) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.5386 1.2615 0.41082 0.38645 ## Proportion of Variance 0.5535 0.3721 0.03946 0.03492 ## Cumulative Proportion 0.5535 0.9256 0.96508 1.00000 Hence, the first two axes together already explain over 90% of the data. We could thus decide in a predictive model to only use these two axes. newx &lt;- predict(pcs) Let’s next make a response variable, which is equal to the sum of the 4 explanatory variable with some added noise on top: y &lt;- apply(x,1,sum) + rnorm(nrow(x),0,0.5) d &lt;- as.data.frame(cbind(y=y,newx)) head(d) ## y PC1 PC2 PC3 PC4 ## 1 -1.6267028 -1.5775951 -0.2800516 -0.3727663 -0.35502422 ## 2 0.7772536 0.5864081 0.8836272 0.3464420 -0.04036141 ## 3 -0.2745109 -2.1333463 1.5687629 -0.7284356 0.21787624 ## 4 -1.0983262 -1.1603281 0.5778170 -0.1408519 -0.13538958 ## 5 -0.3078073 -1.0937142 -0.1890704 0.3171644 -0.64608988 ## 6 -0.7859098 -0.8988454 -1.0151972 -0.6220823 -0.40436840 We can use dredge (should we?) to compare all models: library(MuMIn) ## Warning: package &#39;MuMIn&#39; was built under R version 4.2.3 m.1 &lt;- lm(y~.,data=d,na.action=&quot;na.fail&quot;) modSel &lt;- as.data.frame(dredge(m.1)) ## Fixed term is &quot;(Intercept)&quot; options(digits=3) #best models head(modSel,3) ## (Intercept) PC1 PC2 PC3 PC4 df logLik AICc delta weight ## 13 0.0899 NA NA 1.74 1.14 4 -40.3 89.4 0.000 0.355 ## 15 0.0899 NA 0.0816 1.74 1.14 5 -39.4 90.1 0.670 0.254 ## 14 0.0899 -0.0634 NA 1.74 1.14 5 -39.5 90.3 0.856 0.231 # worst models tail(modSel,3) ## (Intercept) PC1 PC2 PC3 PC4 df logLik AICc delta weight ## 3 0.0899 NA 0.0816 NA NA 3 -70.4 147 57.8 9.88e-14 ## 2 0.0899 -0.0634 NA NA NA 3 -70.4 147 57.9 9.62e-14 ## 4 0.0899 -0.0634 0.0816 NA NA 4 -70.1 149 59.7 3.85e-14 This is an interesting finding: the model that performs best is the model that contains the two axes that explain the least amount of variation, while the model that explains worst is the model that contains the two axes that explain the most variation. We can look at this sligthly more indepth: m.12 &lt;- lm(y~PC1+PC2,data=d) summary(m.12) ## ## Call: ## lm(formula = y ~ PC1 + PC2, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1006 -0.6951 -0.0293 0.7041 2.0347 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0899 0.1435 0.63 0.53 ## PC1 -0.0634 0.0942 -0.67 0.50 ## PC2 0.0816 0.1149 0.71 0.48 ## ## Residual standard error: 1.01 on 47 degrees of freedom ## Multiple R-squared: 0.02, Adjusted R-squared: -0.0217 ## F-statistic: 0.479 on 2 and 47 DF, p-value: 0.622 m.34 &lt;- lm(y~PC3+PC4,data=d) summary(m.34) ## ## Call: ## lm(formula = y ~ PC3 + PC4, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1886 -0.3907 -0.0126 0.3651 1.2402 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0899 0.0790 1.14 0.26 ## PC3 1.7429 0.1942 8.98 9.3e-12 *** ## PC4 1.1439 0.2064 5.54 1.3e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.558 on 47 degrees of freedom ## Multiple R-squared: 0.703, Adjusted R-squared: 0.69 ## F-statistic: 55.6 on 2 and 47 DF, p-value: 4.05e-13 Hence, we might conclude that at least in this case using a PCA to reduce variables and then fit would not be ideal. That is not to say that it is never a useful approach, I just think that one should be cautious. "],["transforming-posterior-samples.html", "5 Transforming posterior samples 5.1 Binomial model 5.2 Beta zero one inflated model", " 5 Transforming posterior samples 5.1 Binomial model Let’s first try something relatively easy: a binomial model with one random effect. We first generate data: library(brms) set.seed(456) N &lt;- 50 Ni &lt;- 10 x &lt;- rnorm(N) id &lt;- sample(Ni,N,replace=TRUE) id.f &lt;- factor(id) id.vals &lt;- rnorm(Ni) lin.y &lt;- x + id.vals[id] p.y &lt;- plogis(lin.y) y &lt;- rbinom(N,1,p.y) d &lt;- data.frame(x=x,y=y,id=id.f) And then proceed with the analysis: # 1. A bernoulli model with random effect m1 &lt;- brm(y~x+(1|id),family=bernoulli, warmup=1000, iter=2000, data=d, backend=&quot;cmdstanr&quot;, file=&quot;BRMS_bernoulli&quot;) # data points for which we want predictions: newdat &lt;- data.frame(x=c(0.3,0.4),id=&#39;1&#39;) # predictions on linear scale m1.fit.lin &lt;- fitted(m1,summary=FALSE,newdata=newdat,scale = &#39;linear&#39;,re_formula=NA) # define a function for getting estimate, se and 95% CI sum.fun &lt;- function(x) c(mu=mean(x),sd=sd(x),quantile(x,probs=c(0.025,0.975))) # apply function for each row in newdat t(apply(plogis(m1.fit.lin),2,sum.fun)) ## mu sd 2.5% 97.5% ## [1,] 0.5661154 0.1563952 0.2212366 0.8396047 ## [2,] 0.5958201 0.1562524 0.2413899 0.8598464 # taking them directly from brms fitted(m1,summary=TRUE,newdata=newdat,scale = &#39;response&#39;,re_formula=NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.5661154 0.1563952 0.2212366 0.8396047 ## [2,] 0.5958201 0.1562524 0.2413899 0.8598464 Fortunately, we see that both methods give identical estimates. 5.2 Beta zero one inflated model We now move to a slightly more complicated model: a beta regression with zero-one inflation. Assume we have a 40% chance of having inflation, which in turn in 80% of the cases implies a 0 and in 20% of the cases a 1. library(brms) set.seed(456) N &lt;- 50 Ni &lt;- 10 x &lt;- rnorm(N) id &lt;- sample(Ni,N,replace=TRUE) id.f &lt;- factor(id) id.vals &lt;- rnorm(Ni) lin.y &lt;- x + id.vals[id] p.y &lt;- plogis(lin.y) beta.var &lt;- 0.04 # convert according to wikipedia # maybe not the most professional source # but didn&#39;t have time to rederive right now # (nor to look up a better source) shape.1 &lt;- p.y*(p.y*(1-p.y)/beta.var -1) shape.2 &lt;- (1-p.y)*(p.y*(1-p.y)/beta.var -1) y.std &lt;- rbeta(N,1,shape.1,shape.2) # now add zero-one inflation: y &lt;- ifelse(runif(N)&lt;0.4,as.numeric(runif(N)&gt;0.8),y.std) d &lt;- data.frame(x=x,y=y,id=id.f) Now we move to the analysis, first we are naive and we simply try the logistic transformation of the predictions: # 2. A beta zero one inflated model with random effect m2 &lt;- brm(y~x+(1|id),family=zero_one_inflated_beta, warmup=1000, iter=2000, data=d, backend=&quot;cmdstanr&quot;, file=&quot;BRMS_beta_zero_one&quot;) # data points for which we want predictions: newdat &lt;- data.frame(x=c(0.3,0.4),id=&#39;1&#39;) # predictions on linear scale m2.fit.lin &lt;- fitted(m2,summary=FALSE,newdata=newdat,scale = &#39;linear&#39;,re_formula=NA) # define a function for getting estimate, se and 95% CI sum.fun &lt;- function(x) c(mu=mean(x),sd=sd(x),quantile(x,probs=c(0.025,0.975))) # apply function for each row in newdat t(apply(plogis(m2.fit.lin),2,sum.fun)) ## mu sd 2.5% 97.5% ## [1,] 0.5763428 0.08192093 0.4108652 0.7336134 ## [2,] 0.5653001 0.08322229 0.3990678 0.7266214 # taking them directly from brms fitted(m2,summary=TRUE,newdata=newdat,scale = &#39;response&#39;,re_formula=NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4103261 0.06361266 0.2858994 0.5397669 ## [2,] 0.4035142 0.06396549 0.2802933 0.5333942 Now, the two are different, simply because we have not applied the zero one inflation. Let us now try to extract these values: # extract chances of zero one inflation and conditional chance of one from the model: p.zoi &lt;- as_draws_array(m2,variable = c(&#39;zoi&#39;)) p.coi &lt;- as_draws_array(m2,variable = c(&#39;coi&#39;)) # define a function for getting estimate, se and 95% CI sum.fun.zoi &lt;- function(x){ x.corrected &lt;- x*(1-p.zoi) + p.zoi*p.coi c(mu=mean(x.corrected),sd=sd(x.corrected),quantile(x.corrected,probs=c(0.025,0.975))) } # apply function for each row in newdat t(apply(plogis(m2.fit.lin),2,sum.fun.zoi)) ## mu sd 2.5% 97.5% ## [1,] 0.4103261 0.06361266 0.2858994 0.5397669 ## [2,] 0.4035142 0.06396549 0.2802933 0.5333942 # taking them directly from brms fitted(m2,summary=TRUE,newdata=newdat,scale = &#39;response&#39;,re_formula=NA) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 0.4103261 0.06361266 0.2858994 0.5397669 ## [2,] 0.4035142 0.06396549 0.2802933 0.5333942 And now they are identical indeed, as expected! "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
